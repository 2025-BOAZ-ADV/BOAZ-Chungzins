{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ef6b52e2",
      "metadata": {
        "id": "ef6b52e2"
      },
      "source": [
        "#### 환경설정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dc03a42",
      "metadata": {
        "id": "9dc03a42"
      },
      "source": [
        "##### 1. Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f04d7d6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f04d7d6f",
        "outputId": "74b32ba8-525f-4d7f-fc2a-5982ec084f6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/HyeonSeok/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvanillahub12\u001b[0m (\u001b[33mboaz_woony-boaz\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "# wandb 로그인\n",
        "wandb.login(key=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "992382bf",
      "metadata": {
        "id": "992382bf"
      },
      "source": [
        "##### 2. 라이브러리 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5MISAwpScmYt",
      "metadata": {
        "id": "5MISAwpScmYt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8ebed6c5",
      "metadata": {
        "id": "8ebed6c5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torch import Tensor\n",
        "from torchsummary import summary\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d453e5f",
      "metadata": {
        "id": "2d453e5f"
      },
      "source": [
        "##### 3. 경로 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "mSXgKx8GoItj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSXgKx8GoItj",
        "outputId": "63d180e3-66a3-49f8-ab7c-d0a594e25b11"
      },
      "outputs": [],
      "source": [
        "ROOT = \"/home/HyeonSeok/BOAZ-Chungzins/data/raw\"\n",
        "CHECKPOINT_PATH = \"/home/HyeonSeok/BOAZ-Chungzins/save_path/checkpoint\"\n",
        "PICKLE_PATH = \"/home/HyeonSeok/BOAZ-Chungzins/save_path/pickle\"\n",
        "text = \"/home/HyeonSeok/BOAZ-Chungzins/data/metadata/train_test_split.txt\"\n",
        "\n",
        "demo_info = \"/home/HyeonSeok/demographic_info.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecaaf5a1",
      "metadata": {
        "id": "ecaaf5a1"
      },
      "source": [
        "##### 4. Seed 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c9f4e372",
      "metadata": {
        "id": "c9f4e372"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # type: ignore\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
        "    torch.backends.cudnn.benchmark = False  # type: ignore\n",
        "\n",
        "seed_everything(42) # Seed 고정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nydfgBckyPt3",
      "metadata": {
        "id": "nydfgBckyPt3"
      },
      "source": [
        "## 1. Data Load"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b7326d1",
      "metadata": {},
      "source": [
        "#### 1-1. Demographic Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "732dca44",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo_df = pd.read_csv(demo_info, sep=' ', header=None).iloc[:,0:3]\n",
        "demo_df.columns = ['patient', 'age', 'gender']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb8bb89",
      "metadata": {},
      "source": [
        "유일하게 나이, 성별에 결측값이 존재하는 환자 -> 223번 환자는 test data에만 존재"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "66e81126",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "patient    223\n",
              "age        NaN\n",
              "gender     NaN\n",
              "Name: 122, dtype: object"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo_df.loc[122]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af15dd7d",
      "metadata": {},
      "source": [
        "0-18세는 소아청소년, 18세 이상은 성인으로 이진분류"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0d1a1c44",
      "metadata": {},
      "outputs": [],
      "source": [
        "def categorize_age(age):\n",
        "    if age <= 18:\n",
        "        return 1\n",
        "    else:   # NaN도 0에 할당 (0이 최빈값이므로)\n",
        "        return 0\n",
        "\n",
        "demo_df['age'] = demo_df['age'].apply(categorize_age)\n",
        "demo_df.columns = ['patient', 'child', 'gender']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eec81911",
      "metadata": {},
      "source": [
        "NaN 제외, child 48명, adult 77명"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "df9c1402",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "child\n",
              "0    77\n",
              "1    49\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo_df['child'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "118c0fd6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "female\n",
              "0    80\n",
              "1    46\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo_df['gender'] = demo_df['gender'].apply(lambda x: 1 if x=='F' else 0)   # NaN도 0에 할당 (남자가 최빈값이므로)\n",
        "\n",
        "demo_df.columns = ['patient', 'child', 'female']\n",
        "demo_df['female'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "61fc1bf0",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo_dict = demo_df.set_index('patient')[['child', 'female']].apply(list, axis=1).to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "927dcfa6",
      "metadata": {},
      "source": [
        "#### 1.2 Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c3896b75",
      "metadata": {},
      "outputs": [],
      "source": [
        "# WAV 파일이 있는 디렉토리 경로\n",
        "data_dir = ROOT\n",
        "txt_dir = ROOT\n",
        "\n",
        "df = pd.read_csv(text, sep='\\t', header=None)\n",
        "\n",
        "# 컬럼 이름 변경\n",
        "df.columns = ['filename', 'set']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5d0d9a50",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(df)):\n",
        "    filename = int(df.iloc[i,0].split('_')[0])\n",
        "\n",
        "    if demo_dict[filename][0] == 1:\n",
        "        df.loc[i,'child'] = 1\n",
        "    else:\n",
        "        df.loc[i,'child'] = 0\n",
        "    \n",
        "    if demo_dict[filename][1] == 1:\n",
        "        df.loc[i,'female'] = 1\n",
        "    else:\n",
        "        df.loc[i,'female'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d2a52dff",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>set</th>\n",
              "      <th>child</th>\n",
              "      <th>female</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>107_3p2_Tc_mc_AKGC417L</td>\n",
              "      <td>train</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>108_1b1_Al_sc_Meditron</td>\n",
              "      <td>train</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>109_1b1_Al_sc_Litt3200</td>\n",
              "      <td>test</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>109_1b1_Ar_sc_Litt3200</td>\n",
              "      <td>test</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>109_1b1_Ll_sc_Litt3200</td>\n",
              "      <td>test</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>109_1b1_Lr_sc_Litt3200</td>\n",
              "      <td>test</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>109_1b1_Pl_sc_Litt3200</td>\n",
              "      <td>test</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>109_1b1_Pr_sc_Litt3200</td>\n",
              "      <td>test</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>110_1b1_Pr_sc_Meditron</td>\n",
              "      <td>train</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>110_1p1_Al_sc_Meditron</td>\n",
              "      <td>train</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>110_1p1_Ll_sc_Meditron</td>\n",
              "      <td>train</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  filename    set  child  female\n",
              "40  107_3p2_Tc_mc_AKGC417L  train    0.0     1.0\n",
              "41  108_1b1_Al_sc_Meditron  train    1.0     0.0\n",
              "42  109_1b1_Al_sc_Litt3200   test    0.0     1.0\n",
              "43  109_1b1_Ar_sc_Litt3200   test    0.0     1.0\n",
              "44  109_1b1_Ll_sc_Litt3200   test    0.0     1.0\n",
              "45  109_1b1_Lr_sc_Litt3200   test    0.0     1.0\n",
              "46  109_1b1_Pl_sc_Litt3200   test    0.0     1.0\n",
              "47  109_1b1_Pr_sc_Litt3200   test    0.0     1.0\n",
              "48  110_1b1_Pr_sc_Meditron  train    0.0     0.0\n",
              "49  110_1p1_Al_sc_Meditron  train    0.0     0.0\n",
              "50  110_1p1_Ll_sc_Meditron  train    0.0     0.0"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[40:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1fab8667",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train : 539, Test : 381, Total : 920\n"
          ]
        }
      ],
      "source": [
        "# train, test split\n",
        "train_df = df[df['set'] == 'train']\n",
        "test_df = df[df['set'] == 'test']\n",
        "\n",
        "# filename list\n",
        "train_file_list = train_df['filename'].tolist()\n",
        "test_file_list = test_df['filename'].tolist()\n",
        "\n",
        "# age list (각 파일의 환자의 소아 여부)\n",
        "train_age_list = train_df['child'].tolist()\n",
        "test_age_list = test_df['child'].tolist()\n",
        "\n",
        "# gender list (각 파일의 환자의 여성 여부)\n",
        "train_gender_list = train_df['female'].tolist()\n",
        "test_gender_list = test_df['female'].tolist()\n",
        "\n",
        "# list 통합\n",
        "train_list = [list(x) for x in zip(train_file_list, train_age_list, train_gender_list)]\n",
        "test_list = [list(x) for x in zip(test_file_list, test_age_list, test_gender_list)]\n",
        "\n",
        "print(f'Train : {len(train_list)}, Test : {len(test_list)}, Total : {len(train_list) + len(test_list)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oVi6lzuPpSbk",
      "metadata": {
        "id": "oVi6lzuPpSbk"
      },
      "source": [
        "## 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8c7719",
      "metadata": {
        "id": "5e8c7719"
      },
      "source": [
        "#### 2.1 Args"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "634b232e",
      "metadata": {
        "id": "634b232e"
      },
      "source": [
        "        K: queue size; number of negative keys (default: 65536)\n",
        "        m: moco momentum of updating key encoder (default: 0.999)\n",
        "        T: softmax temperature (default: 0.07)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add5c69b",
      "metadata": {
        "id": "add5c69b"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    # Audio & Spectrogram\n",
        "    target_sr = 4000   \n",
        "    frame_size = 1024\n",
        "    hop_length = 512    # frame_size 절반\n",
        "    n_mels = 128\n",
        "    target_sec = 8\n",
        "\n",
        "    # Augmentation\n",
        "    time_mask_param = 0.5\n",
        "    freq_mask_param = 0.5\n",
        "\n",
        "    # Train\n",
        "    lr = 0.03\n",
        "    warm = True                     # warm-up 사용 여부\n",
        "    warm_epochs = 10                # warm-up 적용할 초기 epoch 수\n",
        "    warmup_from = lr * 0.1          # warm-up 시작 learning rate (보통 lr의 10%)\n",
        "    warmup_to = lr\n",
        "\n",
        "    batch_size = 128\n",
        "    workers = 4\n",
        "    epochs = 300\n",
        "    weight_decay = 1e-3\n",
        "\n",
        "    resume = None\n",
        "    schedule=[120, 160] # schedule\n",
        "\n",
        "    # MLS\n",
        "    K = 512\n",
        "    momentum = 0.999\n",
        "    T = 0.07\n",
        "    dim_prj = 64\n",
        "    top_k = 15\n",
        "    lambda_bce = 0.5\n",
        "    out_dim = 512\n",
        "\n",
        "    # Linear Evaluation\n",
        "    ft_epochs = 100\n",
        "\n",
        "    # etc\n",
        "    gpu = 0\n",
        "    data = \"./data_path\"\n",
        "    seed=42\n",
        "\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58e1f949",
      "metadata": {
        "id": "58e1f949"
      },
      "source": [
        "#### 2.2 Utils (func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8d2d1329",
      "metadata": {
        "id": "8d2d1329"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "# cycle의 클래스를 추출\n",
        "def get_class(cr, wh):\n",
        "    if cr == 1 and wh == 1:\n",
        "        return 3\n",
        "    elif cr == 0 and wh == 1:\n",
        "        return 2\n",
        "    elif cr == 1 and wh == 0:\n",
        "        return 1\n",
        "    elif cr == 0 and wh == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "# Mel Spectrogram 생성 ( sr=4KHz, frame_size=1024, hop_length=512, n_mels=128 )\n",
        "def generate_mel_spectrogram(waveform, sample_rate, frame_size, hop_length, n_mels):\n",
        "    if hop_length is None:\n",
        "        hop_length = frame_size // 2\n",
        "    mel_spec_transform = T.MelSpectrogram(\n",
        "        sample_rate=sample_rate,\n",
        "        n_fft=frame_size,\n",
        "        hop_length=hop_length,\n",
        "        n_mels=n_mels\n",
        "    )\n",
        "    mel_spectrogram = mel_spec_transform(waveform)\n",
        "    mel_db = T.AmplitudeToDB()(mel_spectrogram)\n",
        "\n",
        "    # scaling\n",
        "    # mean = mel_db.mean()\n",
        "    # std = mel_db.std() + 1e-6\n",
        "    # mel.db = (mel_db - mean) / std\n",
        "\n",
        "    return mel_db\n",
        "\n",
        "# Cycle Repeat 또는 Crop\n",
        "def repeat_or_truncate_segment(mel_segment, target_frames):\n",
        "    current_frames = mel_segment.shape[-1]\n",
        "    if current_frames >= target_frames:\n",
        "        return mel_segment[:, :, :target_frames]\n",
        "    else:\n",
        "        repeat_ratio = math.ceil(target_frames / current_frames)\n",
        "        mel_segment = mel_segment.repeat(1, 1, repeat_ratio)\n",
        "        return mel_segment[:, :, :target_frames]\n",
        "\n",
        "def preprocess_waveform_segment(waveform, unit_length):\n",
        "\n",
        "    \"\"\"unit_length 기준으로 waveform을 repeat + padding 또는 crop하여 길이 정규화\"\"\"\n",
        "    waveform = waveform.squeeze(0)  # (1, L) → (L,) 로 바꿔도 무방\n",
        "    length_adj = unit_length - len(waveform)\n",
        "\n",
        "    if length_adj > 0:\n",
        "        # waveform이 너무 짧은 경우 → repeat + zero-padding\n",
        "        half_unit = unit_length // 2\n",
        "\n",
        "        if length_adj < half_unit:\n",
        "            # 길이 차이가 작으면 단순 padding\n",
        "            half_adj = length_adj // 2\n",
        "            waveform = F.pad(waveform, (half_adj, length_adj - half_adj))\n",
        "        else:\n",
        "            # 반복 후 부족한 부분 padding\n",
        "            repeat_factor = unit_length // len(waveform)\n",
        "            waveform = waveform.repeat(repeat_factor)[:unit_length]\n",
        "            remaining = unit_length - len(waveform)\n",
        "            half_pad = remaining // 2\n",
        "            waveform = F.pad(waveform, (half_pad, remaining - half_pad))\n",
        "    else:\n",
        "        # waveform이 너무 길면 앞쪽 1/4 내에서 랜덤 crop\n",
        "        length_adj = len(waveform) - unit_length\n",
        "        start = random.randint(0, length_adj // 4)\n",
        "        waveform = waveform[start:start + unit_length]\n",
        "\n",
        "    return waveform.unsqueeze(0)  # 다시 (1, L)로\n",
        "\n",
        "def preprocess_waveform_with_fade_repeat(waveform, unit_length, fade_ratio=0.1):\n",
        "    \"\"\"\n",
        "    길이 unit_length까지 반복하며 fade-in/out으로 연결하는 방식의 padding\n",
        "    waveform: (1, L) or (L,)\n",
        "    fade_ratio: 각 반복 연결부에서 fade-in/out 적용 비율 (0.1 → 10%)\n",
        "    \"\"\"\n",
        "    if waveform.dim() == 2:\n",
        "        waveform = waveform.squeeze(0)  # (1, L) → (L,)\n",
        "\n",
        "    orig_len = len(waveform)\n",
        "    fade_len = int(orig_len * fade_ratio)\n",
        "\n",
        "    if orig_len >= unit_length:\n",
        "        # 너무 길면 crop\n",
        "        length_adj = orig_len - unit_length\n",
        "        start = random.randint(0, length_adj // 4)\n",
        "        waveform = waveform[start:start + unit_length]\n",
        "        return waveform.unsqueeze(0)\n",
        "\n",
        "    # 만들고자 하는 길이만큼 반복\n",
        "    full_wave = waveform.clone()\n",
        "    while len(full_wave) < unit_length:\n",
        "        next_cycle = waveform.clone()\n",
        "\n",
        "        # fade-out 마지막 구간\n",
        "        fade_out = torch.linspace(1.0, 0.0, fade_len)\n",
        "        full_wave[-fade_len:] *= fade_out\n",
        "\n",
        "        # fade-in 앞부분\n",
        "        fade_in = torch.linspace(0.0, 1.0, fade_len)\n",
        "        next_cycle[:fade_len] *= fade_in\n",
        "\n",
        "        # 이어붙이기\n",
        "        full_wave = torch.cat([full_wave, next_cycle], dim=0)\n",
        "\n",
        "    # 최종 길이 맞추기\n",
        "    waveform = full_wave[:unit_length]\n",
        "    return waveform.unsqueeze(0)  # (1, L)\n",
        "\n",
        "# 데이터 Spec Augmentation ( 0~80% Random Masking )\n",
        "def apply_spec_augment(mel_segment):\n",
        "\n",
        "    M = mel_segment.shape[-1]\n",
        "    F = mel_segment.shape[-2]\n",
        "\n",
        "    # torchaudio의 마스킹은 0부터 mask_param까지 균등분포에서 랜덤하게 길이를 선택\n",
        "    time_masking = T.TimeMasking(time_mask_param=int(M * 0.8))\n",
        "    freq_masking = T.FrequencyMasking(freq_mask_param=int(F * 0.8) )\n",
        "\n",
        "    aug1 = freq_masking(mel_segment.clone())\n",
        "    aug2 = time_masking(mel_segment.clone())\n",
        "    aug3 = freq_masking(time_masking(mel_segment.clone()))\n",
        "\n",
        "    return aug1, aug2, aug3\n",
        "\n",
        "# Waveform resample\n",
        "def resample_waveform(waveform, orig_sr, target_sr=args.target_sr):\n",
        "    if orig_sr != target_sr:\n",
        "        resampler = torchaudio.transforms.Resample(\n",
        "            orig_freq=orig_sr,\n",
        "            new_freq=target_sr\n",
        "        )\n",
        "        return resampler(waveform), target_sr\n",
        "    return waveform, orig_sr\n",
        "\n",
        "\n",
        "# Normalize - Mean/Std\n",
        "def get_mean_and_std(dataset):\n",
        "    \"\"\" 전체 mel-spectrogram에서 mean과 std 계산 \"\"\"\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)\n",
        "\n",
        "    cnt = 0\n",
        "    fst_moment = torch.zeros(1)\n",
        "    snd_moment = torch.zeros(1)\n",
        "    for inputs, _, _ in tqdm(dataloader, desc=\"[Calculating Mean/Std]\"):\n",
        "        b, c, h, w = inputs.shape  # inputs: [1, 1, n_mels, time]\n",
        "        nb_pixels = b * h * w\n",
        "\n",
        "        fst_moment += torch.sum(inputs, dim=[0, 2, 3])\n",
        "        snd_moment += torch.sum(inputs**2, dim=[0, 2, 3])\n",
        "        cnt += nb_pixels\n",
        "\n",
        "    mean = fst_moment / cnt\n",
        "    std = torch.sqrt(snd_moment / cnt - mean**2)\n",
        "    return mean.item(), std.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "70af5e6b",
      "metadata": {
        "id": "70af5e6b"
      },
      "outputs": [],
      "source": [
        "##############################################\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchaudio.transforms as T\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# -------------------- Augmentation functions (ICBHI 멜스펙트로그램에 최적화) --------------------\n",
        "\n",
        "def spec_augment(mel, time_mask_ratio=0.15, freq_mask_ratio=0.15):\n",
        "    \"\"\"\n",
        "    SpecAugment: 시간/주파수 영역 마스킹\n",
        "    - 시간축 마스킹: 63 * 0.15 ≈ 9 프레임\n",
        "    - 주파수 마스킹: 128 * 0.1 ≈ 12 채널\n",
        "    \"\"\"\n",
        "    M = mel.shape[-1]  # 시간 축\n",
        "    F = mel.shape[-2]  # 주파수 축\n",
        "\n",
        "    time_masking = T.TimeMasking(time_mask_param=max(1, int(M * time_mask_ratio)))\n",
        "    freq_masking = T.FrequencyMasking(freq_mask_param=max(1, int(F * freq_mask_ratio)))\n",
        "\n",
        "    mel = freq_masking(mel.clone())\n",
        "    mel = time_masking(mel)\n",
        "    return mel\n",
        "\n",
        "def add_noise(mel, noise_level=0.001):\n",
        "    \"\"\"\n",
        "    노이즈 추가: 적당한 수준의 표준 정규분포 노이즈 (너무 높으면 손실 커짐)\n",
        "    \"\"\"\n",
        "    noise = torch.randn_like(mel) * noise_level\n",
        "    return mel + noise\n",
        "\n",
        "def pitch_shift(mel, n_steps=2):\n",
        "    \"\"\"\n",
        "    주파수 축 순환 이동 (mel axis). shape은 그대로 유지됨.\n",
        "    n_steps=2면 ±2 멜 채널만 이동.\n",
        "    \"\"\"\n",
        "    shift = random.randint(-n_steps, n_steps)\n",
        "    if shift == 0:\n",
        "        return mel\n",
        "    if shift > 0:\n",
        "        mel = torch.cat([mel[:, :, shift:, :], mel[:, :, :shift, :]], dim=2)\n",
        "    else:\n",
        "        shift = abs(shift)\n",
        "        mel = torch.cat([mel[:, :, -shift:, :], mel[:, :, :-shift, :]], dim=2)\n",
        "    return mel\n",
        "\n",
        "def time_stretch(mel, min_rate=0.95, max_rate=1.05):\n",
        "    \"\"\"\n",
        "    시간 축 길이 조절. 너무 심하지 않게 ±5% 범위로만 조정.\n",
        "    - shape 유지 위해 interpolation 후 crop/pad\n",
        "    \"\"\"\n",
        "    rate = random.uniform(min_rate, max_rate)\n",
        "    if rate == 1.0:\n",
        "        return mel\n",
        "\n",
        "    orig_size = mel.shape[-1]\n",
        "    target_size = int(orig_size * rate)\n",
        "\n",
        "    mel_stretched = F.interpolate(\n",
        "        mel, size=(mel.shape[-2], target_size),  # (mel_bins, time)\n",
        "        mode='bilinear',\n",
        "        align_corners=False\n",
        "    )\n",
        "\n",
        "    if target_size > orig_size:\n",
        "        return mel_stretched[..., :orig_size]\n",
        "    else:\n",
        "        pad = orig_size - target_size\n",
        "        return F.pad(mel_stretched, (0, pad))\n",
        "\n",
        "# -------------------- Dispatcher --------------------\n",
        "\n",
        "AUGMENTATION_FUNCTIONS_TORCH = {\n",
        "    \"spec_augment\": spec_augment,\n",
        "    \"add_noise\": add_noise,\n",
        "    \"pitch_shift\": pitch_shift,\n",
        "    \"time_stretch\": time_stretch\n",
        "}\n",
        "\n",
        "def apply_augmentations_torch(x, methods=[], **kwargs):\n",
        "    for method in methods:\n",
        "        func = AUGMENTATION_FUNCTIONS_TORCH.get(method)\n",
        "        if func is None:\n",
        "            raise ValueError(f\"Unknown augmentation: {method}\")\n",
        "        x = func(x, **kwargs.get(method, {}))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4a62aa74",
      "metadata": {
        "id": "4a62aa74"
      },
      "outputs": [],
      "source": [
        "def aug(repeat_mel):\n",
        "    # 먼저 복사본 준비\n",
        "    mel1 = repeat_mel.clone()\n",
        "    mel2 = repeat_mel.clone()\n",
        "\n",
        "    # 각각 다른 증강 A, B 적용\n",
        "    aug1 = apply_augmentations_torch(mel1, methods=[\"add_noise\"], add_noise={\"noise_level\": 0.005})\n",
        "    aug2 = apply_augmentations_torch(mel2, methods=[\"time_stretch\"], time_stretch={\"min_rate\": 0.8, \"max_rate\": 1.2})\n",
        "    # aug3 = apply_augmentations_torch(mel3, methods=[\"pitch_shift\"], pitch_shift={\"n_steps\": 2})\n",
        "\n",
        "    # # 각 결과에 spec_augment 추가 적용\n",
        "    aug1_spec = spec_augment(aug1, time_mask_ratio=0.6, freq_mask_ratio=0.4)\n",
        "    aug2_spec = spec_augment(aug2, time_mask_ratio=0.6, freq_mask_ratio=0.4)\n",
        "    # aug3_spec = spec_augment(aug3, time_mask_ratio=0.6, freq_mask_ratio=0.4)\n",
        "\n",
        "    return aug1_spec, aug2_spec, None\n",
        "\n",
        "# classwise하게 cycle_list를 만들지 않고, 모든 cycle_list에서 랜덤으로 샘플링\n",
        "def window_mix(repeat_mel, cycle_list):\n",
        "    \n",
        "    mel1 = repeat_mel.clone()\n",
        "    mel2 = repeat_mel.clone()\n",
        "\n",
        "    B, C, F, T = repeat_mel.shape  # 4D: [B, 1, F, T]\n",
        "\n",
        "    # mel1 증강\n",
        "    for _ in range(random.randint(1, 3)):\n",
        "        window_width = random.randint(int(T * 0.1), int(T * 0.4))\n",
        "        start = random.randint(int(T * 0.1), T - window_width)\n",
        "        end = start + window_width\n",
        "\n",
        "        random_normal_cycle1 = random.choice(cycle_list)[0]\n",
        "        random_normal_cycle1 = random_normal_cycle1.expand(B, -1, -1, -1)\n",
        "        mel1[:, :, :, start:end] = random_normal_cycle1[:, :, :, start:end]\n",
        "\n",
        "    # mel2 증강\n",
        "    for _ in range(random.randint(1, 3)):\n",
        "        window_width = random.randint(int(T * 0.1), int(T * 0.4))\n",
        "        start = random.randint(int(T * 0.1), T - window_width)\n",
        "        end = start + window_width\n",
        "\n",
        "        random_normal_cycle2 = random.choice(cycle_list)[0]\n",
        "        random_normal_cycle2 = random_normal_cycle2.expand(B, -1, -1, -1)\n",
        "        mel2[:, :, :, start:end] = random_normal_cycle2[:, :, :, start:end]\n",
        "\n",
        "    return mel1, mel2, None\n",
        "\n",
        "\n",
        "def get_timestamp():\n",
        "    \"\"\"Outputs current time in KST like 2404070830\"\"\"\n",
        "    kst_time = datetime.now(ZoneInfo(\"Asia/Seoul\"))\n",
        "    return kst_time.strftime('%y%m%d%H%M')\n",
        "\n",
        "# Origin\n",
        "# def aug(repeat_mel):\n",
        "#     aug1, aug2, aug3 = apply_spec_augment(repeat_mel)\n",
        "#     return aug1, aug2, aug3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39e684cb",
      "metadata": {
        "id": "39e684cb"
      },
      "source": [
        "#### 2.3 CycleDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1642a79a",
      "metadata": {
        "id": "1642a79a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "class CycleDataset(Dataset):\n",
        "    def __init__(self, file_list, wav_dir, txt_dir, target_sec=args.target_sec, target_sr=args.target_sr, frame_size=args.frame_size, hop_length=args.hop_length, n_mels=args.n_mels):\n",
        "        self.file_list = file_list\n",
        "        self.wav_dir = wav_dir\n",
        "        self.txt_dir = txt_dir\n",
        "        self.target_sec = target_sec\n",
        "        self.target_sr = target_sr\n",
        "        self.frame_size = frame_size\n",
        "        self.hop_length = hop_length\n",
        "        self.n_mels = n_mels\n",
        "\n",
        "        self.cycle_list = []\n",
        "\n",
        "        print(\"[INFO] Preprocessing cycles...\")\n",
        "        for filename, child, female in tqdm(self.file_list):\n",
        "            txt_path = os.path.join(self.txt_dir, filename + '.txt')\n",
        "            wav_path = os.path.join(self.wav_dir, filename + '.wav')\n",
        "\n",
        "            if not os.path.exists(txt_path):\n",
        "                print(f\"[WARNING] Missing file: {txt_path}\")\n",
        "            if not os.path.exists(wav_path):\n",
        "                print(f\"[WARNING] Missing file: {wav_path}\")\n",
        "\n",
        "            # Load annotation\n",
        "            cycle_data = np.loadtxt(txt_path, usecols=(0, 1))\n",
        "            lung_label = np.loadtxt(txt_path, usecols=(2, 3))\n",
        "\n",
        "            # Load waveform\n",
        "            waveform, orig_sr = torchaudio.load(wav_path)\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = torch.mean(waveform, dim=0, keepdim=True)  # Stereo to mono\n",
        "\n",
        "            # Resample to target sample rate (4kHz)\n",
        "            waveform, sample_rate = resample_waveform(waveform, orig_sr, self.target_sr)\n",
        "\n",
        "            for idx in range(len(cycle_data)):\n",
        "                # 호흡 주기 start, end\n",
        "                start_sample = int(cycle_data[idx, 0] * sample_rate)\n",
        "                end_sample = int(cycle_data[idx, 1] * sample_rate)\n",
        "                lung_duration = cycle_data[idx, 1] - cycle_data[idx, 0]\n",
        "\n",
        "                if end_sample <= start_sample:\n",
        "                    continue  # 잘못된 구간 스킵\n",
        "\n",
        "                # Waveform repeat + padding 후 Mel_db\n",
        "                cycle_wave = waveform[:, start_sample:end_sample]\n",
        "                normed_wave = preprocess_waveform_with_fade_repeat(cycle_wave, unit_length=int(self.target_sec * self.target_sr))\n",
        "                mel = generate_mel_spectrogram(normed_wave, sample_rate, frame_size=self.frame_size, hop_length=self.hop_length, n_mels=self.n_mels)\n",
        "\n",
        "                # crackle, wheeze -> class\n",
        "                cr = int(lung_label[idx, 0])\n",
        "                wh = int(lung_label[idx, 1])\n",
        "                label = get_class(cr, wh)\n",
        "\n",
        "                multi_label = torch.tensor([\n",
        "                    float(label in [1, 3]),\n",
        "                    float(label in [2, 3])\n",
        "                ])  # 변환된 multi-label 반환\n",
        "\n",
        "                # meta_data (나이, 성별 정보를 추가하였음 - 0720)\n",
        "                meta_data = (filename, lung_duration, child, female)\n",
        "\n",
        "                self.cycle_list.append((mel, multi_label, meta_data))\n",
        "\n",
        "        print(f\"[INFO] Total cycles collected: {len(self.cycle_list)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cycle_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mel, label, meta_data = self.cycle_list[idx]\n",
        "        return mel, label, meta_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "076531c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def multilabel_to_class(label_tensor):\n",
        "#     crackle = label_tensor[0].item()\n",
        "#     wheeze = label_tensor[1].item()\n",
        "#     if crackle and wheeze:\n",
        "#         return 3  # Both\n",
        "#     elif crackle:\n",
        "#         return 1  # Crackle\n",
        "#     elif wheeze:\n",
        "#         return 2  # Wheeze\n",
        "#     else:\n",
        "#         return 0  # Normal\n",
        "\n",
        "# def group_by_class_multilabel(cycle_list):\n",
        "#     from collections import defaultdict\n",
        "\n",
        "#     classwise_dict = defaultdict(list)\n",
        "\n",
        "#     for sample in cycle_list:\n",
        "#         label_tensor = sample[1]\n",
        "#         cls = multilabel_to_class(label_tensor)\n",
        "#         classwise_dict[cls].append(sample)\n",
        "\n",
        "#     return classwise_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55d5a070",
      "metadata": {
        "id": "55d5a070"
      },
      "source": [
        "##### Pickle.dump"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "752088df",
      "metadata": {
        "id": "752088df"
      },
      "source": [
        "CycleDataset 객체 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "9d386e82",
      "metadata": {
        "id": "9d386e82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Preprocessing cycles...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 539/539 [00:11<00:00, 45.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Total cycles collected: 4142\n",
            "[INFO] Preprocessing cycles...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 381/381 [00:07<00:00, 48.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Total cycles collected: 2756\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "wav_dir = ROOT\n",
        "txt_dir = ROOT\n",
        "\n",
        "# 1. Dataset 로드\n",
        "train_dataset = CycleDataset(train_list, wav_dir, txt_dir)\n",
        "test_dataset = CycleDataset(test_list, wav_dir, txt_dir)\n",
        "\n",
        "# train_dataset 셔플\n",
        "random.shuffle(train_dataset.cycle_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4BQgVyGnrDbN",
      "metadata": {
        "id": "4BQgVyGnrDbN"
      },
      "source": [
        "pickle로 train_dataset, test_dataset 외부 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7a273735",
      "metadata": {
        "id": "7a273735"
      },
      "outputs": [],
      "source": [
        "pickle_name = f'MLS_age_gen_fade_normall_{args.target_sr//1000}kHz_{args.frame_size}win_{args.hop_length}hop_{args.n_mels}mel_{args.target_sec}s'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "cd34caa0",
      "metadata": {
        "id": "cd34caa0"
      },
      "outputs": [],
      "source": [
        "pickle_dict = {\n",
        "    'train_dataset': train_dataset,\n",
        "    'test_dataset': test_dataset\n",
        "}\n",
        "\n",
        "save_path = os.path.join(PICKLE_PATH, pickle_name + '.pkl')\n",
        "with open(save_path, 'wb') as f:\n",
        "    pickle.dump(pickle_dict, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "zRqSwthYTtxq",
      "metadata": {
        "id": "zRqSwthYTtxq"
      },
      "outputs": [],
      "source": [
        "# # 2. 간단 통계\n",
        "# print(f\"Total cycles: {len(train_dataset)}\")\n",
        "\n",
        "# label_counter = [0] * 4  # normal, crackle, wheeze, both\n",
        "# for _, multi_label,_ in train_dataset:\n",
        "#     if torch.equal(multi_label, torch.tensor([0., 0.])):\n",
        "#         label_counter[0] += 1\n",
        "#     elif torch.equal(multi_label, torch.tensor([1., 0.])):\n",
        "#         label_counter[1] += 1\n",
        "#     elif torch.equal(multi_label, torch.tensor([0., 1.])):\n",
        "#         label_counter[2] += 1\n",
        "#     elif torch.equal(multi_label, torch.tensor([1., 1.])):\n",
        "#         label_counter[3] += 1\n",
        "\n",
        "# for idx, count in enumerate(label_counter):\n",
        "#     print(f\"Class {idx}: {count} cycles\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yJPtbC3BrqAE",
      "metadata": {
        "id": "yJPtbC3BrqAE"
      },
      "source": [
        "##### Pickle.load\n",
        "저장된 train_dataset, test_dataset을 로드  \n",
        "(> Aug 는 Moco 모델에서 사용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "EWrjdCFSrmER",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWrjdCFSrmER",
        "outputId": "6b437dce-f371-4bbc-c198-643ab1dc1c5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Train] Cycles: 4142\n",
            "[Test] Cycles: 2756\n"
          ]
        }
      ],
      "source": [
        "save_path = os.path.join(PICKLE_PATH, pickle_name + '.pkl')\n",
        "with open(save_path, 'rb') as f:\n",
        "    pickle_dict = pickle.load(f)\n",
        "\n",
        "train_dataset = pickle_dict['train_dataset']\n",
        "test_dataset = pickle_dict['test_dataset']\n",
        "\n",
        "print(f\"[Train] Cycles: {len(train_dataset)}\")\n",
        "print(f\"[Test] Cycles: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "72ee2597",
      "metadata": {},
      "outputs": [],
      "source": [
        "wav_dir = ROOT\n",
        "txt_dir = ROOT\n",
        "\n",
        "# classwise 분리\n",
        "# mel_train_classwise_dict = group_by_class_multilabel(train_dataset.cycle_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "dde6d690",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Calculating Mean/Std]: 100%|██████████| 4142/4142 [00:09<00:00, 442.80it/s]\n"
          ]
        }
      ],
      "source": [
        "mean, std = get_mean_and_std(train_dataset)\n",
        "mean, std = mean, std"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcca3481",
      "metadata": {
        "id": "bcca3481"
      },
      "source": [
        "#### 2.4 DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cce2c3c8",
      "metadata": {
        "id": "cce2c3c8"
      },
      "source": [
        "코드 실행 환경에 따라 num_workers를 적절한 값으로 지정해주세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "8iNQDqY0Su9h",
      "metadata": {
        "id": "8iNQDqY0Su9h"
      },
      "outputs": [],
      "source": [
        "seed_everything(42)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    num_workers=4,\n",
        "    drop_last=True,\n",
        "    pin_memory=True,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    num_workers=4,\n",
        "    drop_last=False,\n",
        "    pin_memory=True,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b492ad67",
      "metadata": {
        "id": "b492ad67"
      },
      "source": [
        "label 분포 확인 (단순 참고용, 실제 환경에서는 pretrain set의 label 분포가 어떤지 알 수 없음)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "fea9d290",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fea9d290",
        "outputId": "7c7940fe-2d7c-4085-9cb9-3a43d1ec925e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train sample: 4142\n",
            "Train label distribution: Counter({0: 2063, 1: 1215, 2: 501, 3: 363})\n",
            "\n",
            "Test sample: 2756\n",
            "Test label distribution: Counter({0: 1579, 1: 649, 2: 385, 3: 143})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# label\n",
        "labels = torch.stack([multi_label for _, multi_label, _ in train_dataset])\n",
        "\n",
        "# test 데이터셋의 라벨 분포 출력\n",
        "train_labels = torch.stack([multi_label for _, multi_label, _ in train_dataset])\n",
        "train_labels_class = (\n",
        "    train_labels[:, 0].long() * 1 +  # crackle bit → *1\n",
        "    train_labels[:, 1].long() * 2    # wheeze bit  → *2\n",
        ")  # [N] shape, values in {0, 1, 2, 3}\n",
        "\n",
        "# test 데이터셋의 라벨 분포 출력\n",
        "test_labels = torch.stack([multi_label for _, multi_label, _ in test_dataset])\n",
        "test_labels_class = (\n",
        "    test_labels[:, 0].long() * 1 +  # crackle bit → *1\n",
        "    test_labels[:, 1].long() * 2    # wheeze bit  → *2\n",
        ")  # [N] shape, values in {0, 1, 2, 3}\n",
        "\n",
        "\n",
        "print(f\"Train sample: {len(train_labels_class)}\")\n",
        "print(\"Train label distribution:\", Counter(train_labels_class.tolist()))\n",
        "print(f\"\\nTest sample: {len(test_labels_class)}\")\n",
        "print(\"Test label distribution:\", Counter(test_labels_class.tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed559ff1",
      "metadata": {
        "id": "ed559ff1"
      },
      "source": [
        "## 3. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca710799",
      "metadata": {
        "id": "ca710799"
      },
      "source": [
        "#### 3.1 Pre-trained ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "2caf4a6c",
      "metadata": {
        "id": "2caf4a6c"
      },
      "outputs": [],
      "source": [
        "# def backbone_resnet():\n",
        "#     # 1. 기본 ResNet50 생성 (pretrained=False로 시작)\n",
        "#     resnet = models.resnet50(pretrained=False)\n",
        "\n",
        "#     # 2. 첫 번째 conv 레이어를 1채널용으로 수정\n",
        "#     resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "#     # 먼저 fc 제거\n",
        "#     resnet.fc = nn.Identity()\n",
        "\n",
        "#     # 3. ImageNet 가중치 로드 (conv1 제외)\n",
        "#     state_dict = load_state_dict_from_url(\n",
        "#         'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "#         progress=True\n",
        "#     )\n",
        "#     if 'conv1.weight' in state_dict:\n",
        "#         del state_dict['conv1.weight']\n",
        "#     resnet.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "#     return resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SbHFZiQNQaBB",
      "metadata": {
        "id": "SbHFZiQNQaBB"
      },
      "source": [
        "ResNet34"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "i9H1NPXgQVKV",
      "metadata": {
        "id": "i9H1NPXgQVKV"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "from torch.hub import load_state_dict_from_url\n",
        "import torch.nn as nn\n",
        "\n",
        "def backbone_resnet():\n",
        "    # 1. 기본 ResNet34 생성\n",
        "    resnet = models.resnet34(pretrained=False)\n",
        "\n",
        "    # 2. 첫 번째 conv 레이어를 1채널용으로 수정\n",
        "    resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "    # fc 제거\n",
        "    resnet.fc = nn.Identity()\n",
        "\n",
        "    # 3. ImageNet 가중치 로드 (conv1 제외)\n",
        "    state_dict = load_state_dict_from_url(\n",
        "        'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "        progress=True\n",
        "    )\n",
        "    if 'conv1.weight' in state_dict:\n",
        "        del state_dict['conv1.weight']\n",
        "    resnet.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    return resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K7EFbqsBSk9E",
      "metadata": {
        "id": "K7EFbqsBSk9E"
      },
      "source": [
        "ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "KQtvFgzaSjTO",
      "metadata": {
        "id": "KQtvFgzaSjTO"
      },
      "outputs": [],
      "source": [
        "# from torchvision import models\n",
        "# from torch.hub import load_state_dict_from_url\n",
        "# import torch.nn as nn\n",
        "\n",
        "# def backbone_resnet():\n",
        "#     # 1. 기본 ResNet18 생성\n",
        "#     resnet = models.resnet18(pretrained=False)\n",
        "\n",
        "#     # 2. 첫 번째 conv 레이어를 1채널용으로 수정\n",
        "#     resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "#     # fc 제거\n",
        "#     resnet.fc = nn.Identity()\n",
        "\n",
        "#     # 3. ImageNet 가중치 로드 (conv1 제외)\n",
        "#     state_dict = load_state_dict_from_url(\n",
        "#         'https://download.pytorch.org/models/resnet18-f37072fd.pth',\n",
        "#         progress=True\n",
        "#     )\n",
        "#     if 'conv1.weight' in state_dict:\n",
        "#         del state_dict['conv1.weight']\n",
        "#     resnet.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "#     return resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "09f21daa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09f21daa",
        "outputId": "08fc745f-c109-4d85-cbe4-4e55638aca18"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/HyeonSeok/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/HyeonSeok/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 64, 112, 32]           3,136\n",
            "       BatchNorm2d-2          [-1, 64, 112, 32]             128\n",
            "              ReLU-3          [-1, 64, 112, 32]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 16]               0\n",
            "            Conv2d-5           [-1, 64, 56, 16]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 16]             128\n",
            "              ReLU-7           [-1, 64, 56, 16]               0\n",
            "            Conv2d-8           [-1, 64, 56, 16]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 16]             128\n",
            "             ReLU-10           [-1, 64, 56, 16]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 16]               0\n",
            "           Conv2d-12           [-1, 64, 56, 16]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 16]             128\n",
            "             ReLU-14           [-1, 64, 56, 16]               0\n",
            "           Conv2d-15           [-1, 64, 56, 16]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 16]             128\n",
            "             ReLU-17           [-1, 64, 56, 16]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 16]               0\n",
            "           Conv2d-19           [-1, 64, 56, 16]          36,864\n",
            "      BatchNorm2d-20           [-1, 64, 56, 16]             128\n",
            "             ReLU-21           [-1, 64, 56, 16]               0\n",
            "           Conv2d-22           [-1, 64, 56, 16]          36,864\n",
            "      BatchNorm2d-23           [-1, 64, 56, 16]             128\n",
            "             ReLU-24           [-1, 64, 56, 16]               0\n",
            "       BasicBlock-25           [-1, 64, 56, 16]               0\n",
            "           Conv2d-26           [-1, 128, 28, 8]          73,728\n",
            "      BatchNorm2d-27           [-1, 128, 28, 8]             256\n",
            "             ReLU-28           [-1, 128, 28, 8]               0\n",
            "           Conv2d-29           [-1, 128, 28, 8]         147,456\n",
            "      BatchNorm2d-30           [-1, 128, 28, 8]             256\n",
            "           Conv2d-31           [-1, 128, 28, 8]           8,192\n",
            "      BatchNorm2d-32           [-1, 128, 28, 8]             256\n",
            "             ReLU-33           [-1, 128, 28, 8]               0\n",
            "       BasicBlock-34           [-1, 128, 28, 8]               0\n",
            "           Conv2d-35           [-1, 128, 28, 8]         147,456\n",
            "      BatchNorm2d-36           [-1, 128, 28, 8]             256\n",
            "             ReLU-37           [-1, 128, 28, 8]               0\n",
            "           Conv2d-38           [-1, 128, 28, 8]         147,456\n",
            "      BatchNorm2d-39           [-1, 128, 28, 8]             256\n",
            "             ReLU-40           [-1, 128, 28, 8]               0\n",
            "       BasicBlock-41           [-1, 128, 28, 8]               0\n",
            "           Conv2d-42           [-1, 128, 28, 8]         147,456\n",
            "      BatchNorm2d-43           [-1, 128, 28, 8]             256\n",
            "             ReLU-44           [-1, 128, 28, 8]               0\n",
            "           Conv2d-45           [-1, 128, 28, 8]         147,456\n",
            "      BatchNorm2d-46           [-1, 128, 28, 8]             256\n",
            "             ReLU-47           [-1, 128, 28, 8]               0\n",
            "       BasicBlock-48           [-1, 128, 28, 8]               0\n",
            "           Conv2d-49           [-1, 128, 28, 8]         147,456\n",
            "      BatchNorm2d-50           [-1, 128, 28, 8]             256\n",
            "             ReLU-51           [-1, 128, 28, 8]               0\n",
            "           Conv2d-52           [-1, 128, 28, 8]         147,456\n",
            "      BatchNorm2d-53           [-1, 128, 28, 8]             256\n",
            "             ReLU-54           [-1, 128, 28, 8]               0\n",
            "       BasicBlock-55           [-1, 128, 28, 8]               0\n",
            "           Conv2d-56           [-1, 256, 14, 4]         294,912\n",
            "      BatchNorm2d-57           [-1, 256, 14, 4]             512\n",
            "             ReLU-58           [-1, 256, 14, 4]               0\n",
            "           Conv2d-59           [-1, 256, 14, 4]         589,824\n",
            "      BatchNorm2d-60           [-1, 256, 14, 4]             512\n",
            "           Conv2d-61           [-1, 256, 14, 4]          32,768\n",
            "      BatchNorm2d-62           [-1, 256, 14, 4]             512\n",
            "             ReLU-63           [-1, 256, 14, 4]               0\n",
            "       BasicBlock-64           [-1, 256, 14, 4]               0\n",
            "           Conv2d-65           [-1, 256, 14, 4]         589,824\n",
            "      BatchNorm2d-66           [-1, 256, 14, 4]             512\n",
            "             ReLU-67           [-1, 256, 14, 4]               0\n",
            "           Conv2d-68           [-1, 256, 14, 4]         589,824\n",
            "      BatchNorm2d-69           [-1, 256, 14, 4]             512\n",
            "             ReLU-70           [-1, 256, 14, 4]               0\n",
            "       BasicBlock-71           [-1, 256, 14, 4]               0\n",
            "           Conv2d-72           [-1, 256, 14, 4]         589,824\n",
            "      BatchNorm2d-73           [-1, 256, 14, 4]             512\n",
            "             ReLU-74           [-1, 256, 14, 4]               0\n",
            "           Conv2d-75           [-1, 256, 14, 4]         589,824\n",
            "      BatchNorm2d-76           [-1, 256, 14, 4]             512\n",
            "             ReLU-77           [-1, 256, 14, 4]               0\n",
            "       BasicBlock-78           [-1, 256, 14, 4]               0\n",
            "           Conv2d-79           [-1, 256, 14, 4]         589,824\n",
            "      BatchNorm2d-80           [-1, 256, 14, 4]             512\n",
            "             ReLU-81           [-1, 256, 14, 4]               0\n",
            "           Conv2d-82           [-1, 256, 14, 4]         589,824\n",
            "      BatchNorm2d-83           [-1, 256, 14, 4]             512\n",
            "             ReLU-84           [-1, 256, 14, 4]               0\n",
            "       BasicBlock-85           [-1, 256, 14, 4]               0\n",
            "           Conv2d-86           [-1, 256, 14, 4]         589,824\n",
            "      BatchNorm2d-87           [-1, 256, 14, 4]             512\n",
            "             ReLU-88           [-1, 256, 14, 4]               0\n",
            "           Conv2d-89           [-1, 256, 14, 4]         589,824\n",
            "      BatchNorm2d-90           [-1, 256, 14, 4]             512\n",
            "             ReLU-91           [-1, 256, 14, 4]               0\n",
            "       BasicBlock-92           [-1, 256, 14, 4]               0\n",
            "           Conv2d-93           [-1, 256, 14, 4]         589,824\n",
            "      BatchNorm2d-94           [-1, 256, 14, 4]             512\n",
            "             ReLU-95           [-1, 256, 14, 4]               0\n",
            "           Conv2d-96           [-1, 256, 14, 4]         589,824\n",
            "      BatchNorm2d-97           [-1, 256, 14, 4]             512\n",
            "             ReLU-98           [-1, 256, 14, 4]               0\n",
            "       BasicBlock-99           [-1, 256, 14, 4]               0\n",
            "          Conv2d-100            [-1, 512, 7, 2]       1,179,648\n",
            "     BatchNorm2d-101            [-1, 512, 7, 2]           1,024\n",
            "            ReLU-102            [-1, 512, 7, 2]               0\n",
            "          Conv2d-103            [-1, 512, 7, 2]       2,359,296\n",
            "     BatchNorm2d-104            [-1, 512, 7, 2]           1,024\n",
            "          Conv2d-105            [-1, 512, 7, 2]         131,072\n",
            "     BatchNorm2d-106            [-1, 512, 7, 2]           1,024\n",
            "            ReLU-107            [-1, 512, 7, 2]               0\n",
            "      BasicBlock-108            [-1, 512, 7, 2]               0\n",
            "          Conv2d-109            [-1, 512, 7, 2]       2,359,296\n",
            "     BatchNorm2d-110            [-1, 512, 7, 2]           1,024\n",
            "            ReLU-111            [-1, 512, 7, 2]               0\n",
            "          Conv2d-112            [-1, 512, 7, 2]       2,359,296\n",
            "     BatchNorm2d-113            [-1, 512, 7, 2]           1,024\n",
            "            ReLU-114            [-1, 512, 7, 2]               0\n",
            "      BasicBlock-115            [-1, 512, 7, 2]               0\n",
            "          Conv2d-116            [-1, 512, 7, 2]       2,359,296\n",
            "     BatchNorm2d-117            [-1, 512, 7, 2]           1,024\n",
            "            ReLU-118            [-1, 512, 7, 2]               0\n",
            "          Conv2d-119            [-1, 512, 7, 2]       2,359,296\n",
            "     BatchNorm2d-120            [-1, 512, 7, 2]           1,024\n",
            "            ReLU-121            [-1, 512, 7, 2]               0\n",
            "      BasicBlock-122            [-1, 512, 7, 2]               0\n",
            "AdaptiveAvgPool2d-123            [-1, 512, 1, 1]               0\n",
            "        Identity-124                  [-1, 512]               0\n",
            "================================================================\n",
            "Total params: 21,278,400\n",
            "Trainable params: 21,278,400\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 27.52\n",
            "Params size (MB): 81.17\n",
            "Estimated Total Size (MB): 108.74\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# summary 함수 사용: (채널, 높이, 너비) 크기를 지정\n",
        "summary(backbone_resnet().to(device), input_size=(1, 224, 64))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "drRVCaSiYo8i",
      "metadata": {
        "id": "drRVCaSiYo8i"
      },
      "source": [
        "#### 3.1 Other Bacbones"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZXV5zCCSfnBg",
      "metadata": {
        "id": "ZXV5zCCSfnBg"
      },
      "source": [
        "DenseNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "o0Scakd0Ysge",
      "metadata": {
        "id": "o0Scakd0Ysge"
      },
      "outputs": [],
      "source": [
        "def backbone_densenet121():\n",
        "    # 1. DenseNet121 구조만 (pretrained=False)\n",
        "    densenet = models.densenet121(pretrained=False)\n",
        "\n",
        "    # 2. 첫번째 conv 레이어를 1채널로 교체\n",
        "    old_conv = densenet.features.conv0\n",
        "    new_conv = nn.Conv2d(\n",
        "        1, old_conv.out_channels,\n",
        "        kernel_size=old_conv.kernel_size,\n",
        "        stride=old_conv.stride,\n",
        "        padding=old_conv.padding,\n",
        "        bias=(old_conv.bias is not None)\n",
        "    )\n",
        "    densenet.features.conv0 = new_conv\n",
        "\n",
        "    # 3. ImageNet 가중치 불러오기 (conv0 제외)\n",
        "    state_dict = load_state_dict_from_url(\n",
        "        'https://download.pytorch.org/models/densenet121-a639ec97.pth', progress=True\n",
        "    )\n",
        "    # conv0 (features.conv0.weight) 삭제\n",
        "    if 'features.conv0.weight' in state_dict:\n",
        "        del state_dict['features.conv0.weight']\n",
        "    densenet.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    densenet.classifier = nn.Identity()\n",
        "\n",
        "    return densenet\n",
        "\n",
        "def backbone_densenet161():\n",
        "    # 1. DenseNet161 구조만 (pretrained=False)\n",
        "    densenet = models.densenet161(pretrained=False)\n",
        "\n",
        "    # 2. 첫번째 conv 레이어를 1채널로 교체\n",
        "    old_conv = densenet.features.conv0\n",
        "    new_conv = nn.Conv2d(\n",
        "        in_channels=1,\n",
        "        out_channels=old_conv.out_channels,\n",
        "        kernel_size=old_conv.kernel_size,\n",
        "        stride=old_conv.stride,\n",
        "        padding=old_conv.padding,\n",
        "        bias=(old_conv.bias is not None)\n",
        "    )\n",
        "    densenet.features.conv0 = new_conv\n",
        "\n",
        "    # 3. ImageNet 가중치 불러오기 (conv0 제외)\n",
        "    state_dict = load_state_dict_from_url(\n",
        "        'https://download.pytorch.org/models/densenet161-8d451a50.pth', progress=True\n",
        "    )\n",
        "    if 'features.conv0.weight' in state_dict:\n",
        "        del state_dict['features.conv0.weight']\n",
        "    densenet.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    # 4. classifier 제거\n",
        "    densenet.classifier = nn.Identity()\n",
        "\n",
        "    return densenet\n",
        "\n",
        "def backbone_densenet201():\n",
        "    # 1. DenseNet201 구조만 (pretrained=False)\n",
        "    densenet = models.densenet201(pretrained=False)\n",
        "\n",
        "    # 2. 첫번째 conv 레이어를 1채널로 교체\n",
        "    old_conv = densenet.features.conv0\n",
        "    new_conv = nn.Conv2d(\n",
        "        in_channels=1,\n",
        "        out_channels=old_conv.out_channels,\n",
        "        kernel_size=old_conv.kernel_size,\n",
        "        stride=old_conv.stride,\n",
        "        padding=old_conv.padding,\n",
        "        bias=(old_conv.bias is not None)\n",
        "    )\n",
        "    densenet.features.conv0 = new_conv\n",
        "\n",
        "    # 3. ImageNet 가중치 불러오기 (conv0 제외)\n",
        "    state_dict = load_state_dict_from_url(\n",
        "        'https://download.pytorch.org/models/densenet201-c1103571.pth', progress=True\n",
        "    )\n",
        "    if 'features.conv0.weight' in state_dict:\n",
        "        del state_dict['features.conv0.weight']\n",
        "    densenet.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    # 4. classifier 제거\n",
        "    densenet.classifier = nn.Identity()\n",
        "\n",
        "    return densenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "X9NtN9RnaYpt",
      "metadata": {
        "id": "X9NtN9RnaYpt"
      },
      "outputs": [],
      "source": [
        "# !pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "-tJVzKM2ZLJy",
      "metadata": {
        "id": "-tJVzKM2ZLJy"
      },
      "outputs": [],
      "source": [
        "# from torchinfo import summary\n",
        "\n",
        "# model = backbone_densenet121().to(device)\n",
        "# summary(model, input_size=(1, 1, 224, 64))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebf3c06b",
      "metadata": {},
      "source": [
        "GRU + ATT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "137afdfe",
      "metadata": {},
      "outputs": [],
      "source": [
        "class HAN_GRU(nn.Module):\n",
        "    def __init__(self, freq_dim=128, hidden_freq=100, hidden_time=250, output_dim=512):\n",
        "        super(HAN_GRU, self).__init__()\n",
        "        self.hidden_freq = hidden_freq\n",
        "        self.hidden_time = hidden_time\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Bidirectional GRU (주파수 영역)\n",
        "        self.freq_gru = nn.GRU(input_size=freq_dim, hidden_size=hidden_freq,\n",
        "                               batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Attention for frequency\n",
        "        self.freq_attn_fc = nn.Linear(hidden_freq * 2, hidden_freq * 2)\n",
        "        self.freq_context_vector = nn.Parameter(torch.randn(hidden_freq * 2))\n",
        "\n",
        "        # Bidirectional GRU (시간 영역)\n",
        "        self.time_gru = nn.GRU(input_size=hidden_freq * 2, hidden_size=hidden_time,\n",
        "                               batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Attention for time\n",
        "        self.time_attn_fc = nn.Linear(hidden_time * 2, hidden_time * 2)\n",
        "        self.time_context_vector = nn.Parameter(torch.randn(hidden_time * 2))\n",
        "\n",
        "        # 마지막 출력 벡터 차원 맞추기\n",
        "        self.fc_out = nn.Linear(hidden_time * 2, output_dim)\n",
        "\n",
        "    def attention(self, rnn_output, attn_fc, context_vector, mask=None):\n",
        "        u = torch.tanh(attn_fc(rnn_output))  # [B, T, D]\n",
        "        attn_scores = torch.matmul(u, context_vector)  # [B, T]\n",
        "\n",
        "        if mask is not None:\n",
        "            # mask가 0인 (padding) 위치의 attention score를 -1e9로 설정 → softmax 이후 거의 0이 됨\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores, dim=1)  # [B, T]\n",
        "        attn_output = torch.sum(rnn_output * attn_weights.unsqueeze(-1), dim=1)  # [B, D]\n",
        "        return attn_output\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # x: (B, C=1, F, T) → squeeze channel\n",
        "        x = x.squeeze(1)   # (B, F, T)\n",
        "        x = x.permute(0, 2, 1)  # (B, T, F)\n",
        "\n",
        "        # Frequency GRU\n",
        "        freq_output, _ = self.freq_gru(x)  # (B, T, 2*hidden_freq)\n",
        "        freq_attn_output = self.attention(freq_output, self.freq_attn_fc, self.freq_context_vector, mask)  # (B, 2*hidden_freq)\n",
        "\n",
        "        # 시간 축을 따라 Attention-GRU\n",
        "        time_input = freq_output  # (B, T, 2*hidden_freq)\n",
        "        time_output, _ = self.time_gru(time_input)  # (B, T, 2*hidden_time)\n",
        "        time_attn_output = self.attention(time_output, self.time_attn_fc, self.time_context_vector, mask)  # (B, 2*hidden_time)\n",
        "\n",
        "        # 최종 임베딩 차원으로 투사\n",
        "        # out = self.fc_out(time_attn_output)  # (B, output_dim)\n",
        "        return time_attn_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "96fb7b52",
      "metadata": {},
      "outputs": [],
      "source": [
        "def backbone_han_gru():\n",
        "    \"\"\"\n",
        "    MoCo에 사용 가능한 GRU + 계층적 Attention 기반 백본 모델 정의.\n",
        "    - 입력: log-mel spectrogram 형태 (B, 1, F, T)\n",
        "    - 출력: 512차원 feature vector\n",
        "    \"\"\"\n",
        "    return HAN_GRU()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0305c74",
      "metadata": {
        "id": "e0305c74"
      },
      "source": [
        "#### 3.2 MoCo (MLS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "ska5GunlcKzI",
      "metadata": {
        "id": "ska5GunlcKzI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# K: queue_g의 크기\n",
        "# dim_enc: projector 통과 전, g1,g2 벡터의 차원\n",
        "# dim_prj: projector 통과 후, z1,z2 벡터의 차원\n",
        "class MoCo(nn.Module):\n",
        "    def __init__(self, base_encoder, dim_enc=args.out_dim, dim_prj=64, K=512, m=0.999, T=0.07, top_k=15, lambda_bce=0.5):\n",
        "        super().__init__()\n",
        "        self.K = K\n",
        "        self.m = m\n",
        "        self.T = T\n",
        "        self.top_k = top_k\n",
        "        self.lambda_bce = lambda_bce\n",
        "\n",
        "        self.encoder_q = base_encoder()\n",
        "        self.encoder_k = base_encoder()\n",
        "\n",
        "        dim_enc = dim_enc\n",
        "        self.proj_head_q = nn.Sequential(\n",
        "            nn.Linear(dim_enc, dim_enc),\n",
        "            nn.BatchNorm1d(dim_enc),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim_enc, dim_prj)\n",
        "        )\n",
        "        self.proj_head_k = nn.Sequential(\n",
        "            nn.Linear(dim_enc, dim_enc),\n",
        "            nn.BatchNorm1d(dim_enc),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim_enc, dim_prj)\n",
        "        )\n",
        "\n",
        "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
        "            param_k.data.copy_(param_q.data)\n",
        "            param_k.requires_grad = False\n",
        "\n",
        "        self.register_buffer(\"queue_g\", F.normalize(torch.randn(dim_enc, K), dim=0))      # g2를 정규화한 후 열 단위로 Qg에 저장\n",
        "        self.register_buffer(\"queue_z\", F.normalize(torch.randn(dim_prj, K), dim=0))      # z2를 정규화한 후 열 단위로 Qz에 저장\n",
        "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))               # 현재 queue에 새로 쓸 위치(인덱스)를 추적하는 포인터 역할\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _momentum_update_key_encoder(self):\n",
        "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
        "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _dequeue_and_enqueue(self, g2, z2):\n",
        "        batch_size = g2.shape[0]\n",
        "        ptr = int(self.queue_ptr)\n",
        "        assert self.K % batch_size == 0\n",
        "        self.queue_g[:, ptr:ptr+batch_size] = g2.T.detach()\n",
        "        self.queue_z[:, ptr:ptr+batch_size] = z2.T.detach()\n",
        "        self.queue_ptr[0] = (ptr + batch_size) % self.K\n",
        "\n",
        "    def forward(self, im_q, im_k, epoch=None, warmup_epochs=10):\n",
        "        # encoder_q → g1 (feature)\n",
        "        g1 = F.normalize(self.encoder_q(im_q), dim=1)  # shape: [B, 2048]\n",
        "\n",
        "        # projection head → z1\n",
        "        z1 = F.normalize(self.proj_head_q(g1), dim=1)  # shape: [B, 128]\n",
        "\n",
        "        # encoder k\n",
        "        with torch.no_grad():\n",
        "            self._momentum_update_key_encoder()\n",
        "            g2 = F.normalize(self.encoder_k(im_k), dim=1)\n",
        "            z2 = F.normalize(self.proj_head_k(g2), dim=1)\n",
        "\n",
        "        # top-k mining\n",
        "        sim_g = torch.matmul(g1, self.queue_g.clone().detach())  # [N, K]\n",
        "        # Ablation(1-1) Hard top-k\n",
        "        topk_idx = torch.topk(sim_g, self.top_k, dim=1).indices\n",
        "        y = torch.zeros_like(sim_g)\n",
        "        y.scatter_(1, topk_idx, 1.0)\n",
        "        # # Ablation(1-2) Soft top-k\n",
        "        # topk_sim, topk_idx = torch.topk(sim_g, self.top_k, dim=1)\n",
        "        # y = torch.zeros_like(sim_g)\n",
        "        # y.scatter_(1, topk_idx, F.softmax(topk_sim / self.T, dim=1))\n",
        "\n",
        "        ##################################################################\n",
        "        # logits from z1 · Qz\n",
        "        sim_z = torch.matmul(z1, self.queue_z.clone().detach())\n",
        "        # Ablation(2-1) BCE Loss\n",
        "        bce_loss = F.binary_cross_entropy_with_logits(sim_z / self.T, y) # 개선-> sigmoid(sim_z), 1/D\n",
        "\n",
        "        # # Ablation(2-2) Weighted BCE Loss\n",
        "        # pos_weight = torch.ones_like(sim_z) * (self.K / self.top_k)\n",
        "        # bce_loss = F.binary_cross_entropy_with_logits(sim_z / self.T, y, pos_weight=pos_weight)\n",
        "        # # Ablation(2-3) another Weighted BCE Loss (비추, top-k만 보는 느낌)\n",
        "        # raw_loss = F.binary_cross_entropy_with_logits(sim_z / self.T, y, reduction='none')  # shape: [B, K]\n",
        "        # bce_loss = raw_loss.sum() / (y.sum() + 1e-6)\n",
        "\n",
        "        ###################################################################\n",
        "        # InfoNCE loss\n",
        "        l_pos = torch.sum(z1 * z2, dim=1, keepdim=True)\n",
        "        l_neg = torch.matmul(z1, self.queue_z.clone().detach())\n",
        "        logits = torch.cat([l_pos, l_neg], dim=1) / self.T\n",
        "        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(logits.device)\n",
        "        info_nce_loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # Total loss (with optional warmup) # MLS 논문에서는 warmup 아예 안쓴다고 함\n",
        "        if epoch is not None and epoch < warmup_epochs:\n",
        "            loss = info_nce_loss\n",
        "        # else:\n",
        "        loss = info_nce_loss + self.lambda_bce * bce_loss\n",
        "        # print(f\"INFO_NCE: {info_nce_loss}\")\n",
        "        # print(f\"TRIPLET: {triplet_loss}\")\n",
        "        # print(f\"BCE: {bce_loss}\")\n",
        "\n",
        "        self._dequeue_and_enqueue(g2, z2)\n",
        "\n",
        "        return loss, logits, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cc1bf21",
      "metadata": {
        "id": "1cc1bf21"
      },
      "source": [
        "## 4. Pretrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "iWjL-9oSm76P",
      "metadata": {
        "id": "iWjL-9oSm76P"
      },
      "outputs": [],
      "source": [
        "pretrain_project_name = f'SHS_age_gen_fade_res34_PT_top{args.top_k}_{get_timestamp()}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "e745fdd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "e745fdd5",
        "outputId": "d23d5744-7e9a-4e82-dcb1-222e8e9e89b8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/HyeonSeok/wandb/run-20250720_173543-7ba56cpw</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/boaz_woony-boaz/ICBHI_MSL_Ablation_all/runs/7ba56cpw' target=\"_blank\">SHS_age_gen_fade_res34_PT_top15_2507210235</a></strong> to <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_MSL_Ablation_all' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_MSL_Ablation_all' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_MSL_Ablation_all</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_MSL_Ablation_all/runs/7ba56cpw' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_MSL_Ablation_all/runs/7ba56cpw</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Avg Train Loss: 8.0771\n",
            "[Epoch 0 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 0, loss: 8.0771)\n",
            "Epoch 1 | Avg Train Loss: 6.8493\n",
            "[Epoch 1 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 1, loss: 6.8493)\n",
            "Epoch 2 | Avg Train Loss: 6.4527\n",
            "[Epoch 2 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 2, loss: 6.4527)\n",
            "Epoch 3 | Avg Train Loss: 6.3260\n",
            "[Epoch 3 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 3, loss: 6.3260)\n",
            "Epoch 4 | Avg Train Loss: 6.0583\n",
            "[Epoch 4 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 4, loss: 6.0583)\n",
            "Epoch 5 | Avg Train Loss: 5.7364\n",
            "[Epoch 5 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 5, loss: 5.7364)\n",
            "Epoch 6 | Avg Train Loss: 5.3356\n",
            "[Epoch 6 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 6, loss: 5.3356)\n",
            "Epoch 7 | Avg Train Loss: 4.9818\n",
            "[Epoch 7 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 7, loss: 4.9818)\n",
            "Epoch 8 | Avg Train Loss: 4.5858\n",
            "[Epoch 8 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 8, loss: 4.5858)\n",
            "Epoch 9 | Avg Train Loss: 4.4283\n",
            "[Epoch 9 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 9, loss: 4.4283)\n",
            "Epoch 10 | Avg Train Loss: 4.0619\n",
            "[Epoch 10 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 10, loss: 4.0619)\n",
            "Epoch 11 | Avg Train Loss: 3.8796\n",
            "[Epoch 11 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 11, loss: 3.8796)\n",
            "Epoch 12 | Avg Train Loss: 3.6491\n",
            "[Epoch 12 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 12, loss: 3.6491)\n",
            "Epoch 13 | Avg Train Loss: 3.5017\n",
            "[Epoch 13 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 13, loss: 3.5017)\n",
            "Epoch 14 | Avg Train Loss: 3.2918\n",
            "[Epoch 14 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 14, loss: 3.2918)\n",
            "Epoch 15 | Avg Train Loss: 2.9936\n",
            "[Epoch 15 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 15, loss: 2.9936)\n",
            "Epoch 16 | Avg Train Loss: 2.8887\n",
            "[Epoch 16 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 16, loss: 2.8887)\n",
            "Epoch 17 | Avg Train Loss: 2.7495\n",
            "[Epoch 17 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 17, loss: 2.7495)\n",
            "Epoch 18 | Avg Train Loss: 2.6309\n",
            "[Epoch 18 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 18, loss: 2.6309)\n",
            "Epoch 19 | Avg Train Loss: 2.5843\n",
            "[Epoch 19 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 19, loss: 2.5843)\n",
            "Epoch 20 | Avg Train Loss: 2.3103\n",
            "[Epoch 20 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 20, loss: 2.3103)\n",
            "Epoch 21 | Avg Train Loss: 2.2850\n",
            "[Epoch 21 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 21, loss: 2.2850)\n",
            "Epoch 22 | Avg Train Loss: 2.2284\n",
            "[Epoch 22 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 22, loss: 2.2284)\n",
            "Epoch 23 | Avg Train Loss: 2.0167\n",
            "[Epoch 23 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 23, loss: 2.0167)\n",
            "Epoch 24 | Avg Train Loss: 1.9155\n",
            "[Epoch 24 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 24, loss: 1.9155)\n",
            "Epoch 25 | Avg Train Loss: 2.2141\n",
            "[Epoch 25 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 26 | Avg Train Loss: 1.9600\n",
            "[Epoch 26 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 27 | Avg Train Loss: 1.8917\n",
            "[Epoch 27 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 27, loss: 1.8917)\n",
            "Epoch 28 | Avg Train Loss: 1.7230\n",
            "[Epoch 28 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 28, loss: 1.7230)\n",
            "Epoch 29 | Avg Train Loss: 1.6909\n",
            "[Epoch 29 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 29, loss: 1.6909)\n",
            "Epoch 30 | Avg Train Loss: 1.5463\n",
            "[Epoch 30 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 30, loss: 1.5463)\n",
            "Epoch 31 | Avg Train Loss: 1.6919\n",
            "[Epoch 31 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 32 | Avg Train Loss: 1.6261\n",
            "[Epoch 32 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 33 | Avg Train Loss: 1.5343\n",
            "[Epoch 33 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 33, loss: 1.5343)\n",
            "Epoch 34 | Avg Train Loss: 1.3791\n",
            "[Epoch 34 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 34, loss: 1.3791)\n",
            "Epoch 35 | Avg Train Loss: 1.4159\n",
            "[Epoch 35 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 36 | Avg Train Loss: 1.2641\n",
            "[Epoch 36 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 36, loss: 1.2641)\n",
            "Epoch 37 | Avg Train Loss: 1.2919\n",
            "[Epoch 37 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 38 | Avg Train Loss: 1.3443\n",
            "[Epoch 38 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 39 | Avg Train Loss: 1.2741\n",
            "[Epoch 39 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 40 | Avg Train Loss: 1.1665\n",
            "[Epoch 40 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 40, loss: 1.1665)\n",
            "Epoch 41 | Avg Train Loss: 1.2017\n",
            "[Epoch 41 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 42 | Avg Train Loss: 1.0624\n",
            "[Epoch 42 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 42, loss: 1.0624)\n",
            "Epoch 43 | Avg Train Loss: 1.1161\n",
            "[Epoch 43 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 44 | Avg Train Loss: 1.0819\n",
            "[Epoch 44 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 45 | Avg Train Loss: 1.0226\n",
            "[Epoch 45 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 45, loss: 1.0226)\n",
            "Epoch 46 | Avg Train Loss: 1.0333\n",
            "[Epoch 46 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 47 | Avg Train Loss: 1.0574\n",
            "[Epoch 47 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 48 | Avg Train Loss: 0.9751\n",
            "[Epoch 48 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 48, loss: 0.9751)\n",
            "Epoch 49 | Avg Train Loss: 0.8956\n",
            "[Epoch 49 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 49, loss: 0.8956)\n",
            "Epoch 50 | Avg Train Loss: 0.9557\n",
            "[Epoch 50 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 51 | Avg Train Loss: 0.9048\n",
            "[Epoch 51 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 52 | Avg Train Loss: 0.8750\n",
            "[Epoch 52 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 52, loss: 0.8750)\n",
            "Epoch 53 | Avg Train Loss: 0.8045\n",
            "[Epoch 53 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 53, loss: 0.8045)\n",
            "Epoch 54 | Avg Train Loss: 0.7993\n",
            "[Epoch 54 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 54, loss: 0.7993)\n",
            "Epoch 55 | Avg Train Loss: 0.7530\n",
            "[Epoch 55 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 55, loss: 0.7530)\n",
            "Epoch 56 | Avg Train Loss: 0.8366\n",
            "[Epoch 56 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 57 | Avg Train Loss: 0.8585\n",
            "[Epoch 57 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 58 | Avg Train Loss: 0.8209\n",
            "[Epoch 58 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 59 | Avg Train Loss: 0.7759\n",
            "[Epoch 59 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 60 | Avg Train Loss: 0.6500\n",
            "[Epoch 60 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 60, loss: 0.6500)\n",
            "Epoch 61 | Avg Train Loss: 0.7182\n",
            "[Epoch 61 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 62 | Avg Train Loss: 0.7691\n",
            "[Epoch 62 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 63 | Avg Train Loss: 0.6544\n",
            "[Epoch 63 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 64 | Avg Train Loss: 0.6810\n",
            "[Epoch 64 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 65 | Avg Train Loss: 0.5991\n",
            "[Epoch 65 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 65, loss: 0.5991)\n",
            "Epoch 66 | Avg Train Loss: 0.6354\n",
            "[Epoch 66 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 67 | Avg Train Loss: 0.6404\n",
            "[Epoch 67 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 68 | Avg Train Loss: 0.6594\n",
            "[Epoch 68 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 69 | Avg Train Loss: 0.6055\n",
            "[Epoch 69 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 70 | Avg Train Loss: 0.5807\n",
            "[Epoch 70 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 70, loss: 0.5807)\n",
            "Epoch 71 | Avg Train Loss: 0.6070\n",
            "[Epoch 71 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 72 | Avg Train Loss: 0.5677\n",
            "[Epoch 72 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 72, loss: 0.5677)\n",
            "Epoch 73 | Avg Train Loss: 0.5427\n",
            "[Epoch 73 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 73, loss: 0.5427)\n",
            "Epoch 74 | Avg Train Loss: 0.5164\n",
            "[Epoch 74 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 74, loss: 0.5164)\n",
            "Epoch 75 | Avg Train Loss: 0.4741\n",
            "[Epoch 75 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 75, loss: 0.4741)\n",
            "Epoch 76 | Avg Train Loss: 0.5461\n",
            "[Epoch 76 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 77 | Avg Train Loss: 0.5232\n",
            "[Epoch 77 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 78 | Avg Train Loss: 0.4714\n",
            "[Epoch 78 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 78, loss: 0.4714)\n",
            "Epoch 79 | Avg Train Loss: 0.4757\n",
            "[Epoch 79 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 80 | Avg Train Loss: 0.5198\n",
            "[Epoch 80 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 81 | Avg Train Loss: 0.4524\n",
            "[Epoch 81 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 81, loss: 0.4524)\n",
            "Epoch 82 | Avg Train Loss: 0.4514\n",
            "[Epoch 82 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 82, loss: 0.4514)\n",
            "Epoch 83 | Avg Train Loss: 0.4251\n",
            "[Epoch 83 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 83, loss: 0.4251)\n",
            "Epoch 84 | Avg Train Loss: 0.4063\n",
            "[Epoch 84 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 84, loss: 0.4063)\n",
            "Epoch 85 | Avg Train Loss: 0.4253\n",
            "[Epoch 85 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 86 | Avg Train Loss: 0.4173\n",
            "[Epoch 86 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 87 | Avg Train Loss: 0.3962\n",
            "[Epoch 87 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 87, loss: 0.3962)\n",
            "Epoch 88 | Avg Train Loss: 0.4097\n",
            "[Epoch 88 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 89 | Avg Train Loss: 0.3903\n",
            "[Epoch 89 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 89, loss: 0.3903)\n",
            "Epoch 90 | Avg Train Loss: 0.4066\n",
            "[Epoch 90 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 91 | Avg Train Loss: 0.3982\n",
            "[Epoch 91 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 92 | Avg Train Loss: 0.4326\n",
            "[Epoch 92 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 93 | Avg Train Loss: 0.4258\n",
            "[Epoch 93 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 94 | Avg Train Loss: 0.3889\n",
            "[Epoch 94 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 94, loss: 0.3889)\n",
            "Epoch 95 | Avg Train Loss: 0.3503\n",
            "[Epoch 95 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 95, loss: 0.3503)\n",
            "Epoch 96 | Avg Train Loss: 0.3830\n",
            "[Epoch 96 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 97 | Avg Train Loss: 0.3463\n",
            "[Epoch 97 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 97, loss: 0.3463)\n",
            "Epoch 98 | Avg Train Loss: 0.3296\n",
            "[Epoch 98 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 98, loss: 0.3296)\n",
            "Epoch 99 | Avg Train Loss: 0.3615\n",
            "[Epoch 99 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "💾 Saved checkpoint to /home/HyeonSeok/BOAZ-Chungzins/save_path/checkpoint/SHS_age_gen_fade_res34_PT_top15_2507210235_099.pth.tar\n",
            "Epoch 100 | Avg Train Loss: 0.3718\n",
            "[Epoch 100 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 101 | Avg Train Loss: 0.3230\n",
            "[Epoch 101 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 101, loss: 0.3230)\n",
            "Epoch 102 | Avg Train Loss: 0.3185\n",
            "[Epoch 102 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 102, loss: 0.3185)\n",
            "Epoch 103 | Avg Train Loss: 0.3101\n",
            "[Epoch 103 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 103, loss: 0.3101)\n",
            "Epoch 104 | Avg Train Loss: 0.3071\n",
            "[Epoch 104 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 104, loss: 0.3071)\n",
            "Epoch 105 | Avg Train Loss: 0.2893\n",
            "[Epoch 105 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 105, loss: 0.2893)\n",
            "Epoch 106 | Avg Train Loss: 0.3001\n",
            "[Epoch 106 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 107 | Avg Train Loss: 0.3005\n",
            "[Epoch 107 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 108 | Avg Train Loss: 0.2995\n",
            "[Epoch 108 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 109 | Avg Train Loss: 0.3054\n",
            "[Epoch 109 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 110 | Avg Train Loss: 0.2784\n",
            "[Epoch 110 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 110, loss: 0.2784)\n",
            "Epoch 111 | Avg Train Loss: 0.2806\n",
            "[Epoch 111 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 112 | Avg Train Loss: 0.2715\n",
            "[Epoch 112 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 112, loss: 0.2715)\n",
            "Epoch 113 | Avg Train Loss: 0.2899\n",
            "[Epoch 113 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 114 | Avg Train Loss: 0.2856\n",
            "[Epoch 114 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 115 | Avg Train Loss: 0.2760\n",
            "[Epoch 115 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 116 | Avg Train Loss: 0.2530\n",
            "[Epoch 116 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 116, loss: 0.2530)\n",
            "Epoch 117 | Avg Train Loss: 0.2549\n",
            "[Epoch 117 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 118 | Avg Train Loss: 0.2416\n",
            "[Epoch 118 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 118, loss: 0.2416)\n",
            "Epoch 119 | Avg Train Loss: 0.2691\n",
            "[Epoch 119 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 120 | Avg Train Loss: 0.2958\n",
            "[Epoch 120 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 121 | Avg Train Loss: 0.2519\n",
            "[Epoch 121 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 122 | Avg Train Loss: 0.2386\n",
            "[Epoch 122 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 122, loss: 0.2386)\n",
            "Epoch 123 | Avg Train Loss: 0.2372\n",
            "[Epoch 123 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 123, loss: 0.2372)\n",
            "Epoch 124 | Avg Train Loss: 0.2443\n",
            "[Epoch 124 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 125 | Avg Train Loss: 0.2336\n",
            "[Epoch 125 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 125, loss: 0.2336)\n",
            "Epoch 126 | Avg Train Loss: 0.2614\n",
            "[Epoch 126 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 127 | Avg Train Loss: 0.2577\n",
            "[Epoch 127 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 128 | Avg Train Loss: 0.2250\n",
            "[Epoch 128 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 128, loss: 0.2250)\n",
            "Epoch 129 | Avg Train Loss: 0.2236\n",
            "[Epoch 129 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 129, loss: 0.2236)\n",
            "Epoch 130 | Avg Train Loss: 0.2135\n",
            "[Epoch 130 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 130, loss: 0.2135)\n",
            "Epoch 131 | Avg Train Loss: 0.2202\n",
            "[Epoch 131 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 132 | Avg Train Loss: 0.2274\n",
            "[Epoch 132 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 133 | Avg Train Loss: 0.2142\n",
            "[Epoch 133 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 134 | Avg Train Loss: 0.2098\n",
            "[Epoch 134 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 134, loss: 0.2098)\n",
            "Epoch 135 | Avg Train Loss: 0.2146\n",
            "[Epoch 135 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 136 | Avg Train Loss: 0.2155\n",
            "[Epoch 136 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 137 | Avg Train Loss: 0.2216\n",
            "[Epoch 137 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 138 | Avg Train Loss: 0.2040\n",
            "[Epoch 138 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 138, loss: 0.2040)\n",
            "Epoch 139 | Avg Train Loss: 0.2119\n",
            "[Epoch 139 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 140 | Avg Train Loss: 0.2036\n",
            "[Epoch 140 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 140, loss: 0.2036)\n",
            "Epoch 141 | Avg Train Loss: 0.2143\n",
            "[Epoch 141 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 142 | Avg Train Loss: 0.2242\n",
            "[Epoch 142 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 143 | Avg Train Loss: 0.2212\n",
            "[Epoch 143 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 144 | Avg Train Loss: 0.1945\n",
            "[Epoch 144 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 144, loss: 0.1945)\n",
            "Epoch 145 | Avg Train Loss: 0.2058\n",
            "[Epoch 145 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 146 | Avg Train Loss: 0.1915\n",
            "[Epoch 146 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 146, loss: 0.1915)\n",
            "Epoch 147 | Avg Train Loss: 0.1863\n",
            "[Epoch 147 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 147, loss: 0.1863)\n",
            "Epoch 148 | Avg Train Loss: 0.1921\n",
            "[Epoch 148 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 149 | Avg Train Loss: 0.1951\n",
            "[Epoch 149 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 150 | Avg Train Loss: 0.2038\n",
            "[Epoch 150 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 151 | Avg Train Loss: 0.1993\n",
            "[Epoch 151 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 152 | Avg Train Loss: 0.1917\n",
            "[Epoch 152 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 153 | Avg Train Loss: 0.1879\n",
            "[Epoch 153 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 154 | Avg Train Loss: 0.1796\n",
            "[Epoch 154 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 154, loss: 0.1796)\n",
            "Epoch 155 | Avg Train Loss: 0.1792\n",
            "[Epoch 155 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 155, loss: 0.1792)\n",
            "Epoch 156 | Avg Train Loss: 0.1773\n",
            "[Epoch 156 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 156, loss: 0.1773)\n",
            "Epoch 157 | Avg Train Loss: 0.1772\n",
            "[Epoch 157 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 157, loss: 0.1772)\n",
            "Epoch 158 | Avg Train Loss: 0.1786\n",
            "[Epoch 158 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 159 | Avg Train Loss: 0.1892\n",
            "[Epoch 159 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 160 | Avg Train Loss: 0.1813\n",
            "[Epoch 160 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 161 | Avg Train Loss: 0.1832\n",
            "[Epoch 161 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 162 | Avg Train Loss: 0.1782\n",
            "[Epoch 162 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 163 | Avg Train Loss: 0.1761\n",
            "[Epoch 163 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 163, loss: 0.1761)\n",
            "Epoch 164 | Avg Train Loss: 0.1782\n",
            "[Epoch 164 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 165 | Avg Train Loss: 0.1795\n",
            "[Epoch 165 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 166 | Avg Train Loss: 0.1811\n",
            "[Epoch 166 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 167 | Avg Train Loss: 0.1799\n",
            "[Epoch 167 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 168 | Avg Train Loss: 0.1800\n",
            "[Epoch 168 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 169 | Avg Train Loss: 0.1705\n",
            "[Epoch 169 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 169, loss: 0.1705)\n",
            "Epoch 170 | Avg Train Loss: 0.1693\n",
            "[Epoch 170 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 170, loss: 0.1693)\n",
            "Epoch 171 | Avg Train Loss: 0.1672\n",
            "[Epoch 171 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 171, loss: 0.1672)\n",
            "Epoch 172 | Avg Train Loss: 0.1701\n",
            "[Epoch 172 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 173 | Avg Train Loss: 0.1665\n",
            "[Epoch 173 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 173, loss: 0.1665)\n",
            "Epoch 174 | Avg Train Loss: 0.1646\n",
            "[Epoch 174 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 174, loss: 0.1646)\n",
            "Epoch 175 | Avg Train Loss: 0.1740\n",
            "[Epoch 175 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 176 | Avg Train Loss: 0.1831\n",
            "[Epoch 176 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 177 | Avg Train Loss: 0.1731\n",
            "[Epoch 177 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 178 | Avg Train Loss: 0.1746\n",
            "[Epoch 178 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 179 | Avg Train Loss: 0.1667\n",
            "[Epoch 179 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 180 | Avg Train Loss: 0.1641\n",
            "[Epoch 180 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 180, loss: 0.1641)\n",
            "Epoch 181 | Avg Train Loss: 0.1695\n",
            "[Epoch 181 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 182 | Avg Train Loss: 0.1638\n",
            "[Epoch 182 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 182, loss: 0.1638)\n",
            "Epoch 183 | Avg Train Loss: 0.1619\n",
            "[Epoch 183 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 183, loss: 0.1619)\n",
            "Epoch 184 | Avg Train Loss: 0.1607\n",
            "[Epoch 184 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 184, loss: 0.1607)\n",
            "Epoch 185 | Avg Train Loss: 0.1748\n",
            "[Epoch 185 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 186 | Avg Train Loss: 0.1662\n",
            "[Epoch 186 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 187 | Avg Train Loss: 0.1652\n",
            "[Epoch 187 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 188 | Avg Train Loss: 0.1653\n",
            "[Epoch 188 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 189 | Avg Train Loss: 0.1628\n",
            "[Epoch 189 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 190 | Avg Train Loss: 0.1589\n",
            "[Epoch 190 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 190, loss: 0.1589)\n",
            "Epoch 191 | Avg Train Loss: 0.1584\n",
            "[Epoch 191 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 191, loss: 0.1584)\n",
            "Epoch 192 | Avg Train Loss: 0.1584\n",
            "[Epoch 192 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 193 | Avg Train Loss: 0.1582\n",
            "[Epoch 193 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 193, loss: 0.1582)\n",
            "Epoch 194 | Avg Train Loss: 0.1594\n",
            "[Epoch 194 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 195 | Avg Train Loss: 0.1676\n",
            "[Epoch 195 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 196 | Avg Train Loss: 0.1606\n",
            "[Epoch 196 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 197 | Avg Train Loss: 0.1586\n",
            "[Epoch 197 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 198 | Avg Train Loss: 0.1613\n",
            "[Epoch 198 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 199 | Avg Train Loss: 0.1595\n",
            "[Epoch 199 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "💾 Saved checkpoint to /home/HyeonSeok/BOAZ-Chungzins/save_path/checkpoint/SHS_age_gen_fade_res34_PT_top15_2507210235_199.pth.tar\n",
            "Epoch 200 | Avg Train Loss: 0.1590\n",
            "[Epoch 200 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 201 | Avg Train Loss: 0.1572\n",
            "[Epoch 201 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 201, loss: 0.1572)\n",
            "Epoch 202 | Avg Train Loss: 0.1573\n",
            "[Epoch 202 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 203 | Avg Train Loss: 0.1578\n",
            "[Epoch 203 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 204 | Avg Train Loss: 0.1579\n",
            "[Epoch 204 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 205 | Avg Train Loss: 0.1622\n",
            "[Epoch 205 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 206 | Avg Train Loss: 0.1577\n",
            "[Epoch 206 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 207 | Avg Train Loss: 0.1578\n",
            "[Epoch 207 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 208 | Avg Train Loss: 0.1581\n",
            "[Epoch 208 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 209 | Avg Train Loss: 0.1570\n",
            "[Epoch 209 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 209, loss: 0.1570)\n",
            "Epoch 210 | Avg Train Loss: 0.1609\n",
            "[Epoch 210 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 211 | Avg Train Loss: 0.1577\n",
            "[Epoch 211 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 212 | Avg Train Loss: 0.1580\n",
            "[Epoch 212 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 213 | Avg Train Loss: 0.1552\n",
            "[Epoch 213 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 213, loss: 0.1552)\n",
            "Epoch 214 | Avg Train Loss: 0.1586\n",
            "[Epoch 214 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 215 | Avg Train Loss: 0.1579\n",
            "[Epoch 215 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 216 | Avg Train Loss: 0.1567\n",
            "[Epoch 216 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 217 | Avg Train Loss: 0.1575\n",
            "[Epoch 217 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 218 | Avg Train Loss: 0.1556\n",
            "[Epoch 218 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 219 | Avg Train Loss: 0.1563\n",
            "[Epoch 219 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 220 | Avg Train Loss: 0.1552\n",
            "[Epoch 220 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 220, loss: 0.1552)\n",
            "Epoch 221 | Avg Train Loss: 0.1568\n",
            "[Epoch 221 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 222 | Avg Train Loss: 0.1603\n",
            "[Epoch 222 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 223 | Avg Train Loss: 0.1577\n",
            "[Epoch 223 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 224 | Avg Train Loss: 0.1549\n",
            "[Epoch 224 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 224, loss: 0.1549)\n",
            "Epoch 225 | Avg Train Loss: 0.1568\n",
            "[Epoch 225 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 226 | Avg Train Loss: 0.1548\n",
            "[Epoch 226 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 226, loss: 0.1548)\n",
            "Epoch 227 | Avg Train Loss: 0.1583\n",
            "[Epoch 227 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 228 | Avg Train Loss: 0.1555\n",
            "[Epoch 228 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 229 | Avg Train Loss: 0.1548\n",
            "[Epoch 229 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 230 | Avg Train Loss: 0.1551\n",
            "[Epoch 230 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 231 | Avg Train Loss: 0.1543\n",
            "[Epoch 231 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 231, loss: 0.1543)\n",
            "Epoch 232 | Avg Train Loss: 0.1545\n",
            "[Epoch 232 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 233 | Avg Train Loss: 0.1534\n",
            "[Epoch 233 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 233, loss: 0.1534)\n",
            "Epoch 234 | Avg Train Loss: 0.1535\n",
            "[Epoch 234 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 235 | Avg Train Loss: 0.1541\n",
            "[Epoch 235 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 236 | Avg Train Loss: 0.1536\n",
            "[Epoch 236 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 237 | Avg Train Loss: 0.1532\n",
            "[Epoch 237 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 237, loss: 0.1532)\n",
            "Epoch 238 | Avg Train Loss: 0.1556\n",
            "[Epoch 238 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 239 | Avg Train Loss: 0.1555\n",
            "[Epoch 239 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 240 | Avg Train Loss: 0.1547\n",
            "[Epoch 240 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 241 | Avg Train Loss: 0.1535\n",
            "[Epoch 241 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 242 | Avg Train Loss: 0.1536\n",
            "[Epoch 242 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 243 | Avg Train Loss: 0.1550\n",
            "[Epoch 243 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 244 | Avg Train Loss: 0.1556\n",
            "[Epoch 244 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 245 | Avg Train Loss: 0.1556\n",
            "[Epoch 245 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 246 | Avg Train Loss: 0.1552\n",
            "[Epoch 246 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 247 | Avg Train Loss: 0.1550\n",
            "[Epoch 247 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 248 | Avg Train Loss: 0.1571\n",
            "[Epoch 248 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 249 | Avg Train Loss: 0.1548\n",
            "[Epoch 249 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 250 | Avg Train Loss: 0.1541\n",
            "[Epoch 250 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 251 | Avg Train Loss: 0.1562\n",
            "[Epoch 251 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 252 | Avg Train Loss: 0.1551\n",
            "[Epoch 252 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 253 | Avg Train Loss: 0.1541\n",
            "[Epoch 253 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 254 | Avg Train Loss: 0.1530\n",
            "[Epoch 254 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "=> Saved best checkpoint (epoch: 254, loss: 0.1530)\n",
            "Epoch 255 | Avg Train Loss: 0.1547\n",
            "[Epoch 255 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 256 | Avg Train Loss: 0.1549\n",
            "[Epoch 256 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 257 | Avg Train Loss: 0.1543\n",
            "[Epoch 257 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 258 | Avg Train Loss: 0.1538\n",
            "[Epoch 258 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 259 | Avg Train Loss: 0.1541\n",
            "[Epoch 259 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 260 | Avg Train Loss: 0.1563\n",
            "[Epoch 260 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 261 | Avg Train Loss: 0.1545\n",
            "[Epoch 261 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 262 | Avg Train Loss: 0.1580\n",
            "[Epoch 262 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 263 | Avg Train Loss: 0.1549\n",
            "[Epoch 263 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 264 | Avg Train Loss: 0.1551\n",
            "[Epoch 264 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 265 | Avg Train Loss: 0.1544\n",
            "[Epoch 265 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 266 | Avg Train Loss: 0.1545\n",
            "[Epoch 266 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 267 | Avg Train Loss: 0.1554\n",
            "[Epoch 267 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 268 | Avg Train Loss: 0.1556\n",
            "[Epoch 268 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 269 | Avg Train Loss: 0.1551\n",
            "[Epoch 269 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 270 | Avg Train Loss: 0.1573\n",
            "[Epoch 270 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 271 | Avg Train Loss: 0.1553\n",
            "[Epoch 271 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 272 | Avg Train Loss: 0.1557\n",
            "[Epoch 272 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 273 | Avg Train Loss: 0.1563\n",
            "[Epoch 273 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 274 | Avg Train Loss: 0.1552\n",
            "[Epoch 274 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 275 | Avg Train Loss: 0.1561\n",
            "[Epoch 275 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 276 | Avg Train Loss: 0.1557\n",
            "[Epoch 276 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 277 | Avg Train Loss: 0.1555\n",
            "[Epoch 277 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 278 | Avg Train Loss: 0.1550\n",
            "[Epoch 278 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 279 | Avg Train Loss: 0.1568\n",
            "[Epoch 279 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 280 | Avg Train Loss: 0.1566\n",
            "[Epoch 280 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 281 | Avg Train Loss: 0.1564\n",
            "[Epoch 281 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 282 | Avg Train Loss: 0.1607\n",
            "[Epoch 282 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 283 | Avg Train Loss: 0.1586\n",
            "[Epoch 283 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 284 | Avg Train Loss: 0.1578\n",
            "[Epoch 284 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 285 | Avg Train Loss: 0.1579\n",
            "[Epoch 285 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 286 | Avg Train Loss: 0.1578\n",
            "[Epoch 286 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 287 | Avg Train Loss: 0.1590\n",
            "[Epoch 287 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 288 | Avg Train Loss: 0.1598\n",
            "[Epoch 288 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 289 | Avg Train Loss: 0.1596\n",
            "[Epoch 289 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 290 | Avg Train Loss: 0.1589\n",
            "[Epoch 290 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 291 | Avg Train Loss: 0.1600\n",
            "[Epoch 291 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 292 | Avg Train Loss: 0.1596\n",
            "[Epoch 292 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 293 | Avg Train Loss: 0.1602\n",
            "[Epoch 293 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 294 | Avg Train Loss: 0.1615\n",
            "[Epoch 294 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 295 | Avg Train Loss: 0.1602\n",
            "[Epoch 295 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 296 | Avg Train Loss: 0.1638\n",
            "[Epoch 296 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 297 | Avg Train Loss: 0.1621\n",
            "[Epoch 297 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 298 | Avg Train Loss: 0.1611\n",
            "[Epoch 298 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "Epoch 299 | Avg Train Loss: 0.1679\n",
            "[Epoch 299 | Step 31] im_q: torch.Size([128, 1, 128, 63]), im_k: torch.Size([128, 1, 128, 63])\n",
            "💾 Saved checkpoint to /home/HyeonSeok/BOAZ-Chungzins/save_path/checkpoint/SHS_age_gen_fade_res34_PT_top15_2507210235_299.pth.tar\n"
          ]
        }
      ],
      "source": [
        "# 모델 지정하기 전 seed 고정 필요\n",
        "# seed_everything(args.seed) # Seed 고정\n",
        "\n",
        "wandb.init(\n",
        "    project=\"ICBHI_MSL_Ablation_all\",           # 프로젝트 이름\n",
        "    name=f\"{pretrain_project_name}\", # 실험 이름\n",
        "    config={\n",
        "        \"epochs\": args.ft_epochs,\n",
        "        \"batch_size\": args.batch_size,\n",
        "        \"lr\": args.lr,\n",
        "        \"momentum\": args.momentum,\n",
        "        \"weight_decay\": args.weight_decay,\n",
        "    }\n",
        ")\n",
        "\n",
        "# 1. MoCo 모델 생성\n",
        "model = MoCo(\n",
        "    base_encoder = backbone_resnet,\n",
        "    dim_enc = args.out_dim,\n",
        "    dim_prj = args.dim_prj,\n",
        "    K = args.K,\n",
        "    m = args.momentum,\n",
        "    T = args.T,\n",
        "    top_k = args.top_k,\n",
        "    lambda_bce = args.lambda_bce\n",
        ").cuda()\n",
        "\n",
        "# 2. Optimizer\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), args.lr, weight_decay=args.weight_decay)\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=args.lr,\n",
        "    momentum=0.9,\n",
        "    weight_decay=args.weight_decay,\n",
        "    nesterov=True\n",
        ")\n",
        "\n",
        "# 3. Cosine Scheduler\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=1e-6)\n",
        "\n",
        "# 4. Train\n",
        "# Best loss 초기화\n",
        "best_loss = float('inf')\n",
        "best_epoch = -1\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "    # ===============================\n",
        "    # Training\n",
        "    # ===============================\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    for i, (repeat_mel, label, _) in enumerate(train_loader): # label 여기선 사용 X\n",
        "        # im_q, im_k, _ = aug(repeat_mel)\n",
        "        im_q, im_k, _ = window_mix(repeat_mel, train_dataset.cycle_list)\n",
        "\n",
        "        # scaling augs\n",
        "        im_q = (im_q - mean) / (std + 1e-6)\n",
        "        im_k = (im_k - mean) / (std + 1e-6)\n",
        "\n",
        "        im_q = im_q.cuda(device=args.gpu, non_blocking=True)\n",
        "        im_k = im_k.cuda(device=args.gpu, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss, output, target = model(im_q=im_q, im_k=im_k)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch} | Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"[Epoch {epoch} | Step {i}] im_q: {im_q.shape}, im_k: {im_k.shape}\")\n",
        "\n",
        "    # =====================================\n",
        "    # Scheduler\n",
        "    # =====================================\n",
        "    scheduler.step()\n",
        "\n",
        "    # # =====================================\n",
        "    # Logging with wandb\n",
        "    # =====================================\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    wandb.log({\n",
        "        # \"epoch\": epoch,\n",
        "        \"train_loss\": avg_train_loss,\n",
        "        # \"lr\": current_lr\n",
        "    })\n",
        "\n",
        "    # =====================================\n",
        "    # Checkpoint (Every 100 epochs)\n",
        "    # =====================================\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        ckpt_path = CHECKPOINT_PATH + f\"/{pretrain_project_name}_{epoch:03d}.pth.tar\"\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }, ckpt_path)\n",
        "        print(f\"💾 Saved checkpoint to {ckpt_path}\")\n",
        "\n",
        "    # ===============================\n",
        "    # Save Best Checkpoint\n",
        "    # ===============================\n",
        "    if avg_train_loss < best_loss:\n",
        "        best_loss = avg_train_loss\n",
        "        best_epoch = epoch\n",
        "        best_ckpt_path = CHECKPOINT_PATH + f\"/{pretrain_project_name}_best_checkpoint.pth.tar\"\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'loss': best_loss\n",
        "        }, best_ckpt_path)\n",
        "        print(f\"=> Saved best checkpoint (epoch: {epoch}, loss: {best_loss:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sZSwN7t7l2n5",
      "metadata": {
        "id": "sZSwN7t7l2n5"
      },
      "source": [
        "## 5. Linear Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "id": "2ac017b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "label_weights = torch.tensor([1.1, 1.3, .9, .9]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "id": "935b5f17",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class JointWeightedBCELoss(torch.nn.Module):\n",
        "    def __init__(self, label_weights=label_weights, lambda_joint=20.0):\n",
        "        super(JointWeightedBCELoss, self).__init__()\n",
        "        self.label_weights = label_weights  # [crackle, wheeze, child, female]\n",
        "        self.lambda_joint = lambda_joint\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        \"\"\"\n",
        "        logits: [B, 4] raw model outputs\n",
        "        labels: [B, 4] binary labels (0 or 1)\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. 기본 가중치 BCE 손실 계산\n",
        "        bce_loss = F.binary_cross_entropy_with_logits(logits, labels, reduction='none')  # [B,4]\n",
        "        weighted_loss = bce_loss * self.label_weights                                   # [B,4]\n",
        "        base_loss = weighted_loss.sum(dim=1).mean()                                     # scalar\n",
        "\n",
        "        # 2. crackle & wheeze에 대한 오류 확률 계산\n",
        "        probs = torch.sigmoid(logits)\n",
        "        \n",
        "        # crackle (0번 label), wheeze (1번 label)\n",
        "        crackle_labels = labels[:, 0]\n",
        "        wheeze_labels = labels[:, 1]\n",
        "        crackle_probs = probs[:, 0]\n",
        "        wheeze_probs = probs[:, 1]\n",
        "\n",
        "        # 예측 오류 확률 (높을수록 잘못 예측)\n",
        "        crackle_error = torch.where(crackle_labels == 1, 1 - crackle_probs, crackle_probs)\n",
        "        wheeze_error = torch.where(wheeze_labels == 1, 1 - wheeze_probs, wheeze_probs)\n",
        "\n",
        "        # 3. crackle-wheeze 공동 페널티 (오류 확률 곱 → 동시에 틀릴수록 큼)\n",
        "        joint_loss = crackle_error * wheeze_error  # [B]\n",
        "\n",
        "        # 최종 공동 손실 계산\n",
        "        joint_loss_mean = joint_loss.mean()  # 평균을 사용해 배치 크기와 독립적으로 유지\n",
        "\n",
        "        # 4. 총 손실\n",
        "        total_loss = base_loss + self.lambda_joint * joint_loss_mean\n",
        "\n",
        "        return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "646538df",
      "metadata": {
        "id": "646538df"
      },
      "source": [
        "#### validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "id": "33da3ad9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33da3ad9",
        "outputId": "006848a5-df01-46c1-a32b-d056cfba130b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2756"
            ]
          },
          "execution_count": 279,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "id": "e66818a3",
      "metadata": {
        "id": "e66818a3"
      },
      "outputs": [],
      "source": [
        "def validate(model, val_loader, criterion, device):\n",
        "    import numpy as np\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    total_cr_loss = 0.0\n",
        "    total_wh_loss = 0.0\n",
        "    total_ch_loss = 0.0\n",
        "    total_fe_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for cycle, labels, meta in val_loader:\n",
        "            cycle, labels, meta = cycle.to(device), labels.to(device), meta\n",
        "\n",
        "            # 나이\n",
        "            child = meta[2].unsqueeze(1).to(device)\n",
        "\n",
        "            # 성별\n",
        "            female = meta[3].unsqueeze(1).to(device)\n",
        "\n",
        "            # (crackle, wheeze, child, female)\n",
        "            labels = torch.cat([labels, child, female], dim=1).to(device)\n",
        "\n",
        "            cycle = (cycle - mean) / (std + 1e-6)\n",
        "            outputs = model(cycle)\n",
        "            \n",
        "            if criterion is None:\n",
        "                bce_loss = F.binary_cross_entropy_with_logits(outputs, labels, reduction='none')      # [B, 4]\n",
        "                weighted_loss = bce_loss * label_weights                    # shape: [B, 4]\n",
        "                loss_base = weighted_loss.sum(dim=1).mean()                      # 각 샘플별 sum → batch mean\n",
        "\n",
        "                # crackle, wheeze loss만 추출 (B, 0) and (B, 1)\n",
        "                crackle_loss = bce_loss[:, 0]   # [B]\n",
        "                wheeze_loss = bce_loss[:, 1]    # [B]\n",
        "                child_loss = bce_loss[:, 2]    # [B]\n",
        "                female_loss = bce_loss[:, 3]    # [B]\n",
        "\n",
        "                # joint_loss 계산 (둘 다 어려운 샘플 penalize)\n",
        "                joint_loss = F.relu(crackle_loss*label_weights[0]  + wheeze_loss*label_weights[1] - 1.) ** 2   # [B]\n",
        "                joint_loss_mean = joint_loss.mean()\n",
        "\n",
        "                # --- Final Loss ---\n",
        "                loss = loss_base + joint_loss_mean\n",
        "\n",
        "                # [DEBUG] 각 loss 로깅용\n",
        "                total_cr_loss += crackle_loss.mean()\n",
        "                total_wh_loss += wheeze_loss.mean()\n",
        "                total_ch_loss += child_loss.mean()\n",
        "                total_fe_loss += female_loss.mean()\n",
        "\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).int()  # threshold = 0.5\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds, dim=0).numpy()   # [N, 2]\n",
        "    all_labels = torch.cat(all_labels, dim=0).numpy() # [N, 2]\n",
        "\n",
        "    avg_loss = running_loss / len(val_loader)\n",
        "\n",
        "    test_crackle_loss = total_cr_loss / len(val_loader)\n",
        "    test_wheeze_loss = total_wh_loss / len(val_loader)\n",
        "    test_child_loss = total_ch_loss / len(val_loader)\n",
        "    test_female_loss = total_fe_loss / len(val_loader)\n",
        "\n",
        "    return avg_loss, test_crackle_loss, test_wheeze_loss, test_child_loss, test_female_loss, all_labels, all_preds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3b5fa31",
      "metadata": {
        "id": "d3b5fa31"
      },
      "source": [
        "### Weighted loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "id": "937f36e6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 1, 2, 2])"
            ]
          },
          "execution_count": 281,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.tensor([1,1])\n",
        "b = torch.tensor([2])\n",
        "c = torch.tensor([2])\n",
        "\n",
        "torch.cat([a,b,c],dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "id": "4475c581",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4475c581",
        "outputId": "85beccdf-e6f9-4b87-fbd9-5e6f76cfbd86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class 0 - Positives (1): 792 / 2756 samples\n",
            "Class 1 - Positives (1): 528 / 2756 samples\n",
            "Class 2 - Positives (1): 333 / 2756 samples\n",
            "Class 3 - Positives (1): 525 / 2756 samples\n",
            "Class Weights: tensor([0.8699, 1.3049, 2.0691, 1.3124], device='cuda:0')\n",
            "alpha_norm: tensor([0.1566, 0.2349, 0.3724, 0.2362], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 💡 다중 라벨 예시: targets는 [B, C] binary matrix (e.g., [1, 0, 1, 0])\n",
        "label_list = []\n",
        "\n",
        "# 👇 train_dataset이 (x, multi_label_tensor, _) 형태라고 가정\n",
        "for _, label, meta in test_dataset:\n",
        "\n",
        "        # multi-label\n",
        "        label = label.to(device)\n",
        "\n",
        "        # 나이\n",
        "        child = torch.tensor([meta[2]]).to(device)\n",
        "\n",
        "        # 성별\n",
        "        female = torch.tensor([meta[3]]).to(device)\n",
        "\n",
        "        # (crackle, wheeze, child, female)\n",
        "        labels = torch.cat([label, child, female], dim=0).to(device)\n",
        "\n",
        "        label_list.append(labels) \n",
        "\n",
        "# 전체 label을 합치기\n",
        "all_labels = torch.stack(label_list, dim=0)  # shape: [N, C]\n",
        "num_classes = all_labels.size(1)\n",
        "total_samples = all_labels.size(0)\n",
        "\n",
        "# 클래스별 1의 개수 세기\n",
        "class_counts = all_labels.sum(dim=0)  # shape: [C]\n",
        "class_weights = total_samples / (num_classes * class_counts + 1e-6)  # smoothed\n",
        "\n",
        "# tensor로 변환\n",
        "class_weights_tensor = class_weights.float().to(device)\n",
        "\n",
        "# 🔹 출력\n",
        "for i, count in enumerate(class_counts.tolist()):\n",
        "    print(f\"Class {i} - Positives (1): {int(count)} / {total_samples} samples\")\n",
        "print(f\"Class Weights: {class_weights_tensor}\")\n",
        "\n",
        "alpha_norm = class_weights_tensor / class_weights_tensor.sum()\n",
        "print(f\"alpha_norm: {alpha_norm}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "id": "N6RiJJnHjnRg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6RiJJnHjnRg",
        "outputId": "fcd859eb-70ac-45dd-a04d-72738db6aafe"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "\n",
        "# # ⚙️ 각 클래스의 positive 개수 (from label distribution)\n",
        "# crackle_pos = 262 + 83  # label 1 or 3\n",
        "# wheeze_pos  = 84 + 83   # label 2 or 3\n",
        "\n",
        "# total_samples = 885\n",
        "# num_classes = 2\n",
        "\n",
        "# # ⚖️ 기본 class weight 계산: inverse frequency\n",
        "# class_counts = torch.tensor([crackle_pos, wheeze_pos], dtype=torch.float)\n",
        "# class_weights = total_samples / (num_classes * class_counts + 1e-6)\n",
        "\n",
        "# # ✅ 정규화: sum = 1\n",
        "# alpha_norm = class_weights / class_weights.sum()\n",
        "\n",
        "# # 출력\n",
        "# print(\"Raw Class Weights:\", class_weights)\n",
        "# print(\"Normalized Alpha (sum=1):\", alpha_norm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e58ee2",
      "metadata": {
        "id": "86e58ee2"
      },
      "source": [
        "### Multi-label Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "id": "72d6150e",
      "metadata": {
        "id": "72d6150e"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiLabelFocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha  # Tensor of shape [C], or scalar\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        logits: [B, C] - raw scores\n",
        "        targets: [B, C] - binary or soft labels\n",
        "        \"\"\"\n",
        "        probs = torch.sigmoid(logits)  # [B, C]\n",
        "        ce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')  # [B, C]\n",
        "\n",
        "        pt = probs * targets + (1 - probs) * (1 - targets)  # p_t\n",
        "        focal_weight = (1 - pt) ** self.gamma               # (1 - pt)^γ\n",
        "\n",
        "        loss = focal_weight * ce_loss                       # focal weight 적용\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            alpha_factor = self.alpha * targets + (1 - self.alpha) * (1 - targets)  # [B, C]\n",
        "            loss = alpha_factor * loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "class StableMultiLabelFocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction='mean', eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha  # tensor of shape [C] or None\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        probs = torch.sigmoid(logits)\n",
        "        probs = torch.clamp(probs, min=self.eps, max=1.0 - self.eps)\n",
        "\n",
        "        # Focal weight\n",
        "        pt = probs * targets + (1 - probs) * (1 - targets)\n",
        "        focal_weight = (1 - pt) ** self.gamma\n",
        "\n",
        "        # BCE loss\n",
        "        ce_loss = - (targets * torch.log(probs) + (1 - targets) * torch.log(1 - probs))\n",
        "\n",
        "        loss = focal_weight * ce_loss\n",
        "\n",
        "        # Safe alpha (class weights) application\n",
        "        if self.alpha is not None:\n",
        "            if self.alpha.dim() == 1:\n",
        "                alpha = self.alpha.view(1, -1)  # reshape for broadcasting\n",
        "            else:\n",
        "                alpha = self.alpha\n",
        "            loss = alpha * loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        else:\n",
        "            return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "id": "zTgAvNcjFdzA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTgAvNcjFdzA",
        "outputId": "8a94398b-51c9-4f61-aaf7-eb1154a612e6"
      },
      "outputs": [],
      "source": [
        "# from collections import Counter\n",
        "# import torch\n",
        "\n",
        "# label_dist = Counter({0:456, 1:262, 2:84, 3:83})  # Finetune 분포\n",
        "\n",
        "# # Crackle: (1 + Both), Wheeze: (2 + Both)\n",
        "# n_crackle = label_dist[1] + label_dist[3]  # 262 + 83\n",
        "# n_wheeze  = label_dist[2] + label_dist[3]  # 84 + 83\n",
        "# n_total   = sum(label_dist.values())       # 885\n",
        "\n",
        "# pos_weight = torch.tensor([\n",
        "#     (n_total - n_crackle) / (n_crackle + 1e-6),\n",
        "#     (n_total - n_wheeze) / (n_wheeze + 1e-6)\n",
        "# ], device=device)\n",
        "\n",
        "# print(pos_weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aea74a2d",
      "metadata": {
        "id": "aea74a2d"
      },
      "source": [
        "## Linear Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "id": "Nm8yaHDRZrT1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "Nm8yaHDRZrT1",
        "outputId": "c3eea671-d3e3-4eb3-a9c3-f47d1d2ae512"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "id": "e8c5369e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "e8c5369e",
        "outputId": "a0a2ecb9-c2cd-4dbb-a981-8f210694face"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/HyeonSeok/wandb/run-20250720_195951-86ju4hka</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/boaz_woony-boaz/ICBHI_MSL_Ablation_all/runs/86ju4hka' target=\"_blank\">SHS_age_gen_fade_res34_LE_top15_2507210459</a></strong> to <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_MSL_Ablation_all' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_MSL_Ablation_all' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_MSL_Ablation_all</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_MSL_Ablation_all/runs/86ju4hka' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_MSL_Ablation_all/runs/86ju4hka</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/boaz_woony-boaz/ICBHI_MSL_Ablation_all/runs/86ju4hka?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f3c65a172e0>"
            ]
          },
          "execution_count": 287,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Wandb 정의\n",
        "\n",
        "# import wandb\n",
        "finetune_project_name = f'SHS_age_gen_fade_res34_LE_top{args.top_k}_{get_timestamp()}'\n",
        "\n",
        "wandb.init(\n",
        "    project=\"ICBHI_MSL_Ablation_all\",           # 프로젝트 이름\n",
        "    name=f\"{finetune_project_name}\", # 실험 이름\n",
        "    config={\n",
        "        \"epochs\": args.ft_epochs,\n",
        "        \"batch_size\": args.batch_size,\n",
        "        \"lr\": args.lr,\n",
        "        \"momentum\": args.momentum,\n",
        "        \"weight_decay\": args.weight_decay,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06e2372b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "06e2372b",
        "outputId": "496ceb9b-53c2-4402-8845-cc42ae610591"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Model Load\n",
        "# 위에서부터 했다면\n",
        "load_ckpt_path = CHECKPOINT_PATH + f\"/{pretrain_project_name}_best_checkpoint.pth.tar\"\n",
        "# 중간부터 이어서 한다면\n",
        "# load_ckpt_path = CHECKPOINT_PATH + \"/SHS_aug(T.N)_PT_128bs_top15_0.5ld_2507201907_best_checkpoint.pth.tar\"\n",
        "\n",
        "# 저장 경로\n",
        "save_ckpt_path = CHECKPOINT_PATH+\"/LE_pth\"\n",
        "\n",
        "# 재현성을 위한 시드 재설정\n",
        "seed_everything(args.seed)\n",
        "\n",
        "# MoCo 모델 생성 및 체크포인트 로드\n",
        "model_eval = MoCo(\n",
        "    base_encoder=backbone_resnet,\n",
        "    dim_enc=args.out_dim,\n",
        "    dim_prj=args.dim_prj,\n",
        "    K=args.K,\n",
        "    m=args.momentum,\n",
        "    T=args.T,\n",
        "    top_k=args.top_k,\n",
        "    lambda_bce=args.lambda_bce\n",
        ")\n",
        "\n",
        "checkpoint = torch.load(load_ckpt_path, map_location=device)\n",
        "model_eval.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "# 사전 학습된 encoder 추출\n",
        "encoder = model_eval.encoder_q.to(device)\n",
        "\n",
        "# 3. Fine-tuning을 위한 분류 모델 정의\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!! num_classes = 4로 변경 !!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!! num_classes = 4로 변경 !!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!! num_classes = 4로 변경 !!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!! num_classes = 4로 변경 !!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "class FineTuningModel(nn.Module):\n",
        "    def __init__(self, encoder, out_dim=args.out_dim, num_classes=4):   # (crackle, wheeze, child, female)\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        # 마지막 FC layer를 제외한 encoder의 모든 레이어 freeze\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # 새로운 분류 헤드 추가\n",
        "        self.classifier = nn.Linear(out_dim, num_classes)\n",
        "        # self.classifier = nn.Sequential(\n",
        "        #     nn.Linear(out_dim, out_dim),\n",
        "        #     nn.GELU(),\n",
        "        #     nn.Dropout(0.3),\n",
        "        #     nn.Linear(out_dim, 128),\n",
        "        #     nn.GELU(),\n",
        "        #     nn.Linear(128, num_classes)\n",
        "        # )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.encoder(x)\n",
        "        return self.classifier(features)\n",
        "\n",
        "# 재현성을 위한 시드 재설정\n",
        "seed_everything(args.seed)\n",
        "\n",
        "# 4. 모델, 손실 함수, 옵티마이저 설정\n",
        "model = FineTuningModel(encoder, out_dim = args.out_dim).to(device)\n",
        "##############################\n",
        "\n",
        "# Ablation(3-1) LE -> BCE Loss\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Ablation(3-2) LE -> 임의로 Wheeze에 가중치를 더 주는 BCE Loss\n",
        "criterion = JointWeightedBCELoss()\n",
        "\n",
        "# Ablation(3-3) LE -> Weighted BCE Loss\n",
        "# criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# Ablation(3-4) LE -> Multi-label Focal Loss\n",
        "# criterion = MultiLabelFocalLoss(\n",
        "#     alpha=alpha_norm.to(device),  # 정규화된 값\n",
        "#     gamma=1.0,                    # hard label일 경우\n",
        "#     reduction='mean'\n",
        "# )\n",
        "\n",
        "############################\n",
        "# optimizer = optim.AdamW(model.classifier.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=args.lr,\n",
        "    momentum=0.9,\n",
        "    weight_decay=args.weight_decay,\n",
        "    nesterov=True\n",
        ")\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=args.ft_epochs, eta_min=1e-6)  # Linear Evaluation에서 epochs는 다르게 적용\n",
        "\n",
        "# Best loss 초기화\n",
        "best_loss = float('inf')\n",
        "best_epoch = -1\n",
        "\n",
        "# 5. Linear Evaluation\n",
        "for epoch in range(args.ft_epochs):\n",
        "\n",
        "    # ===============================\n",
        "    # 1. Training\n",
        "    # ===============================\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_predictions = 0.0\n",
        "    correct_predictions = 0.0\n",
        "\n",
        "    total_cr_loss = 0.0\n",
        "    total_wh_loss = 0.0\n",
        "    total_ch_loss = 0.0\n",
        "    total_fe_loss = 0.0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_outputs = []\n",
        "\n",
        "    pbar = tqdm(train_loader, desc='Linear Evaluation')\n",
        "    for i, (cycle, labels, meta) in enumerate(pbar):\n",
        "\n",
        "        # 나이\n",
        "        child = meta[2].unsqueeze(1)\n",
        "\n",
        "        # 성별\n",
        "        female = meta[3].unsqueeze(1)\n",
        "\n",
        "        # (crackle, wheeze, child, female)\n",
        "        labels = torch.cat([labels, child, female], dim=1)   \n",
        "\n",
        "        # Forward pass\n",
        "        cycle = cycle.cuda(args.gpu)\n",
        "        cycle = (cycle - mean) / (std + 1e-6)\n",
        "        labels = labels.cuda(args.gpu)\n",
        "\n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        output = model(cycle)\n",
        "\n",
        "        if criterion is None:\n",
        "            bce_loss = F.binary_cross_entropy_with_logits(output, labels, reduction='none')      # [B, 4]\n",
        "            weighted_loss = bce_loss * label_weights                    # shape: [B, 4]\n",
        "            loss_base = weighted_loss.sum(dim=1).mean()                 # 각 샘플별 sum → batch mean\n",
        "\n",
        "            crackle_loss = bce_loss[:, 0]   # [B]\n",
        "            wheeze_loss = bce_loss[:, 1]    # [B]\n",
        "            child_loss = bce_loss[:, 2]    # [B]\n",
        "            female_loss = bce_loss[:, 3]    # [B]\n",
        "\n",
        "            # joint_loss 계산 (둘 다 어려운 샘플 penalize)\n",
        "            joint_loss = F.relu(crackle_loss*label_weights[0]  + wheeze_loss*label_weights[1] - 1.) ** 2   # [B]\n",
        "            joint_loss_mean = joint_loss.mean()\n",
        "\n",
        "            # --- Final Loss ---\n",
        "            loss = loss_base + joint_loss_mean\n",
        "\n",
        "            # [DEBUG] 각 loss 로깅용\n",
        "            total_cr_loss += crackle_loss.mean()\n",
        "            total_wh_loss += wheeze_loss.mean()\n",
        "            total_ch_loss += child_loss.mean()\n",
        "            total_fe_loss += female_loss.mean()\n",
        "\n",
        "        else:\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss 계산\n",
        "        total_loss += loss.item() # loss : -> float\n",
        "\n",
        "        # 예측값과 실제값 저장 ( Ablation(4-1) threshold )\n",
        "        predicted = (torch.sigmoid(output) > 0.5).float()\n",
        "        all_preds.append(predicted.detach().cpu())\n",
        "        all_labels.append(labels.detach().cpu())\n",
        "        all_outputs.append(output.detach().cpu())\n",
        "\n",
        "    # train loss\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    train_crackle_loss = total_cr_loss / len(train_loader)\n",
        "    train_wheeze_loss = total_wh_loss / len(train_loader)\n",
        "    train_child_loss = total_ch_loss / len(train_loader)\n",
        "    train_female_loss = total_fe_loss / len(train_loader)\n",
        "\n",
        "    # Concatenate\n",
        "    all_preds = torch.cat(all_preds, dim=0).numpy()    # shape: [N, 4]\n",
        "    all_labels = torch.cat(all_labels, dim=0).numpy()  # shape: [N, 4]\n",
        "    all_output = torch.cat(all_outputs, dim=0).numpy()\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss:.4f}\")\n",
        "    \n",
        "    # =====================================\n",
        "    # 2-Edited. Multi-class 민감도/특이도 계산\n",
        "    # =====================================\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import wandb\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "\n",
        "    def multilabel_to_multiclass(y):\n",
        "        # Crackle → 1, Wheeze → 2, Both → 3, None → 0\n",
        "        y = np.array(y)\n",
        "        return y[:, 0] + y[:, 1]*2\n",
        "\n",
        "    def evaluate_multiclass_confusion(y_true, y_pred, class_names=[\"Normal\", \"Wheeze\", \"Crackle\", \"Both\"]):\n",
        "        y_true_cls = multilabel_to_multiclass(y_true)\n",
        "        y_pred_cls = multilabel_to_multiclass(y_pred)\n",
        "\n",
        "        cm = confusion_matrix(y_true_cls, y_pred_cls, labels=[0, 1, 2, 3])\n",
        "\n",
        "        # N_n: 정상 → 정상\n",
        "        N_n = cm[0, 0]\n",
        "        N_total = cm[0].sum()\n",
        "\n",
        "        # 이상 클래스 정답 수: W, C, B\n",
        "        W_total = cm[1].sum()\n",
        "        C_total = cm[2].sum()\n",
        "        B_total = cm[3].sum()\n",
        "\n",
        "        # 각각의 정답 → 정확한 예측만 고려\n",
        "        W_w = cm[1, 1]\n",
        "        C_c = cm[2, 2]\n",
        "        B_b = cm[3, 3]\n",
        "\n",
        "        SP = N_n / (N_total + 1e-6) #spec\n",
        "        SE = (W_w + C_c + B_b) / (W_total + C_total + B_total + 1e-6) #sense\n",
        "\n",
        "        AS = (SP + SE) / 2\n",
        "        HS = 2 * SP * SE / (SP + SE + 1e-6)\n",
        "\n",
        "        return cm, SE, SP, y_true_cls, y_pred_cls\n",
        "\n",
        "    def log_multiclass_conf_matrix_wandb(cm, class_names, sens, spec, normalize, tag):\n",
        "        # Normalize (비율) 옵션\n",
        "        if normalize:\n",
        "            cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
        "            fmt = '.2f'\n",
        "            title = \"Confusion Matrix (Normalized %)\"\n",
        "        else:\n",
        "            fmt = 'd'\n",
        "            title = \"Confusion Matrix (Raw Count)\"\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(7, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues',\n",
        "                    xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
        "\n",
        "        ax.set_xlabel('Predicted')\n",
        "        ax.set_ylabel('True')\n",
        "        ax.set_title(title)\n",
        "\n",
        "        icbhi_score = (sens + spec) / 2\n",
        "        # 우하단에 성능 출력\n",
        "        ax.text(\n",
        "            0.99, 0.15,\n",
        "            f\"Sensitivity: {sens*100:.2f}%\\nSpecificity: {spec*100:.2f}%\\nICBHI Score: {icbhi_score*100:.2f}%\",\n",
        "            ha='right', va='bottom',\n",
        "            transform=plt.gca().transAxes,\n",
        "            fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8)\n",
        "        )\n",
        "\n",
        "        plt.tight_layout()\n",
        "        # wandb.log({tag: wandb.Image(fig)})\n",
        "        # plt.close(fig)\n",
        "        return fig\n",
        "\n",
        "    # 1. 4-class Confusion Matrix 평가\n",
        "    class_names = [\"Normal\", \"Crackle\", \"Wheeze\", \"Both\"]\n",
        "    cm_4x4, finetune_train_sens, finetune_train_spec, y_true_cls, y_pred_cls = evaluate_multiclass_confusion(all_labels, all_preds, class_names)\n",
        "    finetune_icbhi_score = (finetune_train_sens + finetune_train_spec)/2\n",
        "\n",
        "    print(\"4-Class Confusion Matrix:\\n\", cm_4x4)\n",
        "    print(f\"Sensitivity: {finetune_train_sens:.4f}, Specificity: {finetune_train_spec:.4f}, ICBHI Score: {finetune_icbhi_score:.4f}\")\n",
        "\n",
        "\n",
        "    # ===============================\n",
        "    # 3. Validation\n",
        "    # ===============================\n",
        "    test_loss, test_crackle_loss, test_wheeze_loss, test_child_loss, test_female_loss, test_labels, test_preds = validate(\n",
        "        model, test_loader, criterion, device\n",
        "    )\n",
        "\n",
        "    precision = precision_score(test_labels, test_preds, average='macro')\n",
        "    recall = recall_score(test_labels, test_preds, average='macro')\n",
        "    f1 = f1_score(test_labels, test_preds, average='macro')\n",
        "\n",
        "    test_cm_4x4, test_sens, test_spec, test_y_true_cls, test_y_pred_cls = evaluate_multiclass_confusion(test_labels, test_preds)\n",
        "    test_icbhi_score = (test_sens+test_spec)/2\n",
        "\n",
        "    print(\"[Validation] Confusion Matrix:\\n\", test_cm_4x4)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"[VALIDATION] Sensitivity: {test_sens:.4f}, Specificity: {test_spec:.4f}, Avg ICBHI Score: {(test_sens+test_spec)/2:.4f}\")\n",
        "    print(\"##################################################\")\n",
        "\n",
        "\n",
        "    # ===============================\n",
        "    # 4. Confusion Matrix\n",
        "    # ===============================\n",
        "\n",
        "    # 2. Finetune Count Confusion Matrix 시각화\n",
        "    fig_finetune_raw = log_multiclass_conf_matrix_wandb(cm_4x4, class_names, finetune_train_sens, finetune_train_spec, normalize=False, tag=\"finetune_conf_matrix_raw\")\n",
        "    fig_finetune_norm = log_multiclass_conf_matrix_wandb(cm_4x4, class_names, finetune_train_sens, finetune_train_spec, normalize=True, tag=\"finetune_conf_matrix_norm\")\n",
        "\n",
        "    # 3. Test Confusion Matrix 시각화\n",
        "    fig_test_raw = log_multiclass_conf_matrix_wandb(test_cm_4x4, class_names, test_sens, test_spec, normalize=False, tag=\"test_conf_matrix_raw\")\n",
        "    fig_test_norm = log_multiclass_conf_matrix_wandb(test_cm_4x4, class_names, test_sens, test_spec, normalize=True, tag=\"test_conf_matrix_norm\")\n",
        "\n",
        "    # 4. log dictionary 생성\n",
        "    wandb_log_dict = {\n",
        "        \"finetune_conf_matrix_raw\": wandb.Image(fig_finetune_raw),\n",
        "        \"finetune_conf_matrix_norm\": wandb.Image(fig_finetune_norm),\n",
        "        \"test_conf_matrix_raw\": wandb.Image(fig_test_raw),\n",
        "        \"test_conf_matrix_norm\": wandb.Image(fig_test_norm)\n",
        "    }\n",
        "\n",
        "    # =====================================\n",
        "    # 5. Checkpoint (Every 50 epochs)\n",
        "    # =====================================\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        ckpt_path = save_ckpt_path + f\"{finetune_project_name}_{epoch:03d}.pth.tar\"\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }, ckpt_path)\n",
        "        print(f\"💾 Saved checkpoint to {save_ckpt_path}\")\n",
        "\n",
        "    # ===============================\n",
        "    # 6. Save Best Checkpoint\n",
        "    # ===============================\n",
        "    if test_loss < best_loss:\n",
        "        best_loss = test_loss\n",
        "        best_epoch = epoch\n",
        "        best_ckpt_path = save_ckpt_path + f\"{finetune_project_name}_best.pth.tar\"\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'loss': best_loss\n",
        "        }, best_ckpt_path)\n",
        "        print(f\"=> Saved best checkpoint (epoch: {epoch}, loss: {best_loss:.4f})\")\n",
        "\n",
        "\n",
        "        # 🔹 Confusion Matrix Logging for Best\n",
        "        cm_best, sens_best, spec_best,_, _ = evaluate_multiclass_confusion(test_labels, test_preds, class_names)\n",
        "        fig_best_raw = log_multiclass_conf_matrix_wandb(cm_best, class_names, sens_best, spec_best, normalize=False, tag=\"best_test_conf_matrix_raw\")\n",
        "\n",
        "        fig_best_norm = log_multiclass_conf_matrix_wandb(cm_best, class_names, sens_best, spec_best, normalize=True, tag=\"best_test_conf_matrix_norm\")\n",
        "\n",
        "        wandb_log_dict.update({\n",
        "            \"best_test_conf_matrix_raw\": wandb.Image(fig_best_raw),\n",
        "            \"best_test_conf_matrix_norm\": wandb.Image(fig_best_norm)\n",
        "        })\n",
        "\n",
        "\n",
        "    if epoch == args.ft_epochs - 1:\n",
        "        # 🔸 Confusion Matrix Logging for Last Epoch\n",
        "        cm_last, sens_last, spec_last, _, _  = evaluate_multiclass_confusion(test_labels, test_preds, class_names)\n",
        "        fig_last_raw = log_multiclass_conf_matrix_wandb(cm_last, class_names, sens_last, spec_last, normalize=False, tag=\"last_test_conf_matrix_raw\")\n",
        "\n",
        "        fig_last_norm = log_multiclass_conf_matrix_wandb(cm_last, class_names, sens_last, spec_last, normalize=True, tag=\"last_test_conf_matrix_norm\")\n",
        "\n",
        "        wandb_log_dict.update({\n",
        "            \"last_test_conf_matrix_raw\": wandb.Image(fig_last_raw),\n",
        "            \"last_test_conf_matrix_norm\": wandb.Image(fig_last_norm)\n",
        "        })\n",
        "    # =====================================\n",
        "    # 7. Logging with wandb confusion matrix\n",
        "    # =====================================\n",
        "\n",
        "    # step 1. metrics\n",
        "    wandb.log({\n",
        "        # Train metrics\n",
        "        \"Finetune/epoch\": epoch,\n",
        "        \"Finetune/train_loss\": train_loss,\n",
        "        \"Finetune/train_crackle_loss\": train_crackle_loss,\n",
        "        \"Finetune/train_wheeze_loss\": train_wheeze_loss,\n",
        "        \"Finetune/train_child_loss\": train_child_loss,\n",
        "        \"Finetune/train_female_loss\": train_female_loss,\n",
        "        \"Finetune/train_sens\": finetune_train_sens,\n",
        "        \"Finetune/train_spec\": finetune_train_spec,\n",
        "        \"Finetune/icbhi_score\": finetune_icbhi_score,\n",
        "\n",
        "        # Test metrics\n",
        "        \"Test/loss\": test_loss,\n",
        "        \"Test/test_crackle_loss\": test_crackle_loss,\n",
        "        \"Test/test_wheeze_loss\": test_wheeze_loss,\n",
        "        \"Test/test_child_loss\": test_child_loss,\n",
        "        \"Test/test_female_loss\": test_female_loss,\n",
        "        \"Test/sensitivity\": test_sens,\n",
        "        \"Test/specificity\": test_spec,\n",
        "        \"Test/icbhi_score\": test_icbhi_score\n",
        "    })\n",
        "\n",
        "    # step 2. Confusion matrix\n",
        "    wandb.log(wandb_log_dict)\n",
        "\n",
        "    plt.close(fig_finetune_raw)\n",
        "    plt.close(fig_finetune_norm)\n",
        "    plt.close(fig_test_raw)\n",
        "    plt.close(fig_test_norm)\n",
        "    if 'fig_best_raw' in locals(): plt.close(fig_best_raw)\n",
        "    if 'fig_best_norm' in locals(): plt.close(fig_best_norm)\n",
        "    if 'fig_last_raw' in locals(): plt.close(fig_last_raw)\n",
        "    if 'fig_last_norm' in locals(): plt.close(fig_last_norm)\n",
        "\n",
        "    # ===============================\n",
        "    # 8. Scheduler Step\n",
        "    # ===============================\n",
        "    scheduler.step()\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LQVziM_eb4SL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQVziM_eb4SL",
        "outputId": "648956b3-2310-4a65-9bca-8a3c7de8777a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "497/680/539/1040\n",
            "497/1177\n",
            "1040/1579\n",
            "0.5777400165014953/0.6586447114258108\n",
            "0.4843223557129054\n"
          ]
        }
      ],
      "source": [
        "TP = cm_last[1:, 1:].sum()\n",
        "FN = cm_last[1:, 0].sum()\n",
        "FP = cm_last[0, 1:].sum()\n",
        "TN = cm_last[0, 0]\n",
        "print( f\"{TP}/{FN}/{FP}/{TN}\" )\n",
        "\n",
        "sens =FN / (TP + FN + 1e-6)\n",
        "spec = TN / (TN + FP + 1e-6)\n",
        "\n",
        "print(f\"{TP}/{TP + FN}\")\n",
        "print(f\"{TN}/{TN + FP}\")\n",
        "print(f\"{sens}/{spec}\")\n",
        "print(f\"{(0.31+spec)/2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZO5KTgoxl1CN",
      "metadata": {
        "id": "ZO5KTgoxl1CN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r4Gl8GceqNA2",
      "metadata": {
        "id": "r4Gl8GceqNA2"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # sigmoid 적용\n",
        "# sigmoid_output = sigmoid(all_output)  # shape: (N, 2)\n",
        "# all_preds = (sigmoid_output > 0.5).astype(int)  # binary prediction\n",
        "# all_labels = all_labels.astype(int)  # 정수형으로 일치\n",
        "\n",
        "# # 맞춘 것들\n",
        "# correct_mask = np.all(all_preds == all_labels, axis=1)\n",
        "# correct = np.concatenate([sigmoid_output, all_preds, all_labels], axis=1)[correct_mask]\n",
        "\n",
        "# # 틀린 것들\n",
        "# incorrect_mask = ~correct_mask\n",
        "# incorrect_preds = all_preds[incorrect_mask]\n",
        "# incorrect_labels = all_labels[incorrect_mask]\n",
        "# incorrect_sigmoid = sigmoid_output[incorrect_mask]\n",
        "# incorrect_concat = np.concatenate([incorrect_sigmoid, incorrect_preds, incorrect_labels], axis=1)\n",
        "\n",
        "# # 그룹별 필터링\n",
        "# def get_mismatched_by_label(target_label):\n",
        "#     mask = np.all(incorrect_labels == target_label, axis=1)\n",
        "#     return incorrect_concat[mask]\n",
        "\n",
        "# # 각 그룹 추출\n",
        "# wrong_10 = get_mismatched_by_label([1, 0])  # crackle\n",
        "# wrong_01 = get_mismatched_by_label([0, 1])  # wheeze\n",
        "# wrong_11 = get_mismatched_by_label([1, 1])  # both\n",
        "# wrong_00 = get_mismatched_by_label([0, 0])  # normal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mXw6GqeTqzhR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXw6GqeTqzhR",
        "outputId": "b37fe445-d593-4453-fa5b-a7d7a461ee76"
      },
      "outputs": [],
      "source": [
        "# print(\"\\n✅ 맞춘 것들 (예: [sigmoid1, sigmoid2, pred1, pred2, label1, label2])\")\n",
        "# print(correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15BfpFS8j3BR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15BfpFS8j3BR",
        "outputId": "d3e3bb17-cc81-4e6f-ed48-7aa640d30d0d"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# np.set_printoptions(threshold=10000000)\n",
        "# print(np.concatenate([sigmoid(all_output), all_preds, all_labels], axis=1)[:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pIwj_6k_iutp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIwj_6k_iutp",
        "outputId": "738fb012-8d71-4fbf-e81b-3895059a6be4"
      },
      "outputs": [],
      "source": [
        "# all_output[:128]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7BB5kyP1cPDR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BB5kyP1cPDR",
        "outputId": "4695bf6d-1e51-4b2d-c17c-a8d8640d17f0"
      },
      "outputs": [],
      "source": [
        "# len(all_outputs[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.10.15)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
